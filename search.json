[
  {
    "objectID": "posts/2020-04-08-tidy-multilevel/index.html",
    "href": "posts/2020-04-08-tidy-multilevel/index.html",
    "title": "Wrangling multilevel data in the tidyverse",
    "section": "",
    "text": "This is the narrative lesson plan for my 2-hour R workshop, Wrangling Multilevel Data in the R Tidyverse."
  },
  {
    "objectID": "posts/2020-04-08-tidy-multilevel/index.html#a-brief-intro-to-multilevel-structure-its-everywhere",
    "href": "posts/2020-04-08-tidy-multilevel/index.html#a-brief-intro-to-multilevel-structure-its-everywhere",
    "title": "Wrangling multilevel data in the tidyverse",
    "section": "A brief intro to multilevel structure: It’s everywhere!",
    "text": "A brief intro to multilevel structure: It’s everywhere!\nChances are, you’ve dealt with multilevel data before.\nIn the more micro-scale experimental sciences, data take multilevel structure every time you take multiple measurements per research unit, repeated for multiple research units. In psychology research like mine, this means multiple measurements per participant, repeated for multiple participants. This holds true for any set of multiple measurements out of multiple lab setups (petri dishes, vacuum chambers, whatever you use). Observations can be grouped in all of these cases, by whichever participant/lab apparatus/otherwise that they were collected from.\nIn the macro-scale observational (and more often social) sciences, you might have multiple measurements per city, state, or country, repeated for multiple cities/states/countries. Again, this yields data that can be grouped by the locale from which they were collected.\nAnd in any research, if you have a collaborative project across multiple labs where each lab is collecting the same data at their home location, the data now have multilevel structure because they can be grouped by the lab at which they were collected.\nWhen you have multiple measurements per group, across many grouping units, you have multilevel data! It’s important when you do have multilevel data to characterize patterns in the data both within and between units. This is important to do for deep statistical reasons, but for today, these reasons can be summarized by the following:\nIn general, any two observations from the same units are more likely to have something in common with one another than any two observations from different units. We thus benefit from accounting for the fact that variance within a unit is not equivalent to variance between units.\nHere’s an extreme example of cases where the within-unit and between-unit patterns are actively at odds with one another:\n\n\n\n\n\n\n\n\n\nIf you had data that took this structure, where the slope within units was positive, but the overall values between units are scattered in such a way that the slope across units was negative, failing to examine the unit-level relationships before jumping into the overall data analysis might lead you to draw erroneous conclusions!\nMore generally, a particular relationship that holds within the observations of a single unit might look different in another unit, and eyeballing the possible similarities/differences in these relationships is the first step to accounting for them statistically in the future. In this tutorial, we’ll be practicing using tidyverse techniques to do this eyeballing cleanly and efficiently.\nThe content of this tutorial is technically not statistical in nature, but the motivation behind it is inherently statistical, so that’s why I want to take the time to introduce the statistical reasoning, at least conceptually, before we jump into the R.\nAnd importantly, the techniques we practice today are useful for any data analysis you want to repeat over many identically-structured sub-datasets. I’ll demonstrate another one of these applications, bootstrap resampling analysis, at the end of this tutorial.\n\nInstructor’s note: A full exploration of the motivations behind multilevel data analysis is beyond the scope of this tutorial. Today, we will be focusing on data wrangling tools you can use to explore the multilevel-ness of your data, which can lend useful insights no matter how you ultimately decide to analyze your data."
  },
  {
    "objectID": "posts/2020-04-08-tidy-multilevel/index.html#introducing-the-data",
    "href": "posts/2020-04-08-tidy-multilevel/index.html#introducing-the-data",
    "title": "Wrangling multilevel data in the tidyverse",
    "section": "Introducing the data",
    "text": "Introducing the data\nIn this tutorial, we’ll be exploring grocery store sales data of millenials’ favorite fruit: avocados!\n\n\n\nThe fruit that launched a thousand guacamoles?\n\n\nThese data originally come from the Hass Avocado board, via Justin Kiggins on Kaggle. (I downloaded the local copy of these data on 2020-03-25.)\nThe data come from many weekly measures of avocado sales over a few years, collected in many metropolitan areas of the United States. Intuitively, we might expect that avocado sale data might differ based on which region the avocados were sold in. For example, all regions of the US might show similar patterns of avocado sales, but at different levels depending on the average cost of living in that region. Avocados probably sell for cheaper in Dallas, TX, where I grew up, than in New York City, where I live currently, and I’d like to group avocado sales by region so that region-specific avocado prices don’t influence the patterns I see in the data.\n\nA brief note:\nUnless you specifically do research on grocery store avocado prices, it’s unlikely that these data map directly onto the data you deal with in your own research, and that’s okay. It’s important to practice abstracting data cleaning/manipulation and statistical techniques away from the specific data you’re practicing them on, so that you can learn from techniques demonstrated in other disciplines, figure out how they might help you with your own data, and then apply those techniques. Psychologists can learn from economists can learn from biologists, and vice versa. :)"
  },
  {
    "objectID": "posts/2020-04-08-tidy-multilevel/index.html#reading-in-the-data",
    "href": "posts/2020-04-08-tidy-multilevel/index.html#reading-in-the-data",
    "title": "Wrangling multilevel data in the tidyverse",
    "section": "Reading in the data",
    "text": "Reading in the data\nThis blog post’s local copy of the avocado sales CSV can be found in the static website files here. You should be able to download the CSV from that link by right-clicking and downloading a copy to your computer, or if your computer is actively connected to the internet, you can use the URL as the path to data in a readr::read_csv() call.\nIn this post, I can use a relative path because the .Rmd this web page is generated from is saved in the same folder as the avocado data. You will need to use the appropriate URL or path to a local copy of the data when you read_csv() on your own machine, so be aware that copying and pasting the code out of this chunk will probably not work out of the box.\n\navocado_raw &lt;- read_csv(\"avocado.csv\") %&gt;%\n  # When I want to rename columns, I try to leave the raw data UNTOUCHED\n  # to improve reproducibility in case someone else were to download the data\n  # directly from the source I did\n  # as such, all renaming is done on the tibble after I've read it into R\n  rename(row_num = \"...1\",\n         date = \"Date\",\n         avg_price = \"AveragePrice\",\n         total_volume = \"Total Volume\",\n         PLU4046_sold = \"4046\",\n         PLU4225_sold = \"4225\",\n         PLU4770_sold = \"4770\",\n         total_bags = \"Total Bags\",\n         small_bags = \"Small Bags\",\n         large_bags = \"Large Bags\",\n         xlarge_bags = \"XLarge Bags\") %&gt;%\n  # There's a row numbers column written out in the raw csv.\n  # I don't think we need this so I'll select that column out\n  select(-row_num) %&gt;%\n  # For efficiency, I'll store the year as integer\n  mutate(year = lubridate::year(date) %&gt;% as.integer())\n\nLet’s take a peek at the first few rows of the data.\n\navocado_raw\n\n# A tibble: 18,249 × 13\n   date       avg_price total_volume PLU4046_sold PLU4225_sold PLU4770_sold\n   &lt;date&gt;         &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n 1 2015-12-27      1.33       64237.        1037.       54455.         48.2\n 2 2015-12-20      1.35       54877.         674.       44639.         58.3\n 3 2015-12-13      0.93      118220.         795.      109150.        130. \n 4 2015-12-06      1.08       78992.        1132        71976.         72.6\n 5 2015-11-29      1.28       51040.         941.       43838.         75.8\n 6 2015-11-22      1.26       55980.        1184.       48068.         43.6\n 7 2015-11-15      0.99       83454.        1369.       73673.         93.3\n 8 2015-11-08      0.98      109428.         704.      101815.         80  \n 9 2015-11-01      1.02       99811.        1022.       87316.         85.3\n10 2015-10-25      1.07       74339.         842.       64757.        113  \n# ℹ 18,239 more rows\n# ℹ 7 more variables: total_bags &lt;dbl&gt;, small_bags &lt;dbl&gt;, large_bags &lt;dbl&gt;,\n#   xlarge_bags &lt;dbl&gt;, type &lt;chr&gt;, year &lt;int&gt;, region &lt;chr&gt;\n\n\nWe can get a partial codebook of what data these columns contain from Justin Kiggins, the uploader of the original Kaggle dataset:\n\nSome relevant columns in the dataset:\n\ndate - The date of the observation\navg_price - the average price of a single avocado\ntype - conventional or organic\nyear - the year\nregion - the city or region of the observation\ntotal_volume - Total number of avocados sold\nPLUXXXX_sold - Total number of avocados with PLU XXXX sold\n\n\nFinally, we can assume that the *_bags columns measure the total number of variously sized bags of avocados in a given week.\nGoing forward, we’ll treat region as the unit grouping variable. What all regions are there?\n\nunique(avocado_raw$region)\n\n [1] \"Albany\"              \"Atlanta\"             \"BaltimoreWashington\"\n [4] \"Boise\"               \"Boston\"              \"BuffaloRochester\"   \n [7] \"California\"          \"Charlotte\"           \"Chicago\"            \n[10] \"CincinnatiDayton\"    \"Columbus\"            \"DallasFtWorth\"      \n[13] \"Denver\"              \"Detroit\"             \"GrandRapids\"        \n[16] \"GreatLakes\"          \"HarrisburgScranton\"  \"HartfordSpringfield\"\n[19] \"Houston\"             \"Indianapolis\"        \"Jacksonville\"       \n[22] \"LasVegas\"            \"LosAngeles\"          \"Louisville\"         \n[25] \"MiamiFtLauderdale\"   \"Midsouth\"            \"Nashville\"          \n[28] \"NewOrleansMobile\"    \"NewYork\"             \"Northeast\"          \n[31] \"NorthernNewEngland\"  \"Orlando\"             \"Philadelphia\"       \n[34] \"PhoenixTucson\"       \"Pittsburgh\"          \"Plains\"             \n[37] \"Portland\"            \"RaleighGreensboro\"   \"RichmondNorfolk\"    \n[40] \"Roanoke\"             \"Sacramento\"          \"SanDiego\"           \n[43] \"SanFrancisco\"        \"Seattle\"             \"SouthCarolina\"      \n[46] \"SouthCentral\"        \"Southeast\"           \"Spokane\"            \n[49] \"StLouis\"             \"Syracuse\"            \"Tampa\"              \n[52] \"TotalUS\"             \"West\"                \"WestTexNewMexico\"   \n\n\nMost of these regions appear to correspond to specific metropolitan areas. However, I do see some regions in this data that look like supersets of other regions. \"TotalUS\" ought to be a superset of all of the individual metropolitan regions, and, for example, \"California\" should encompass c(\"LosAngeles\", \"Sacramento\", \"SanDiego\", \"SanFrancisco\"). I’m going to go ahead and filter() out all the data that are for multi-metropolitan regions so we can do our best to avoid double-counting.\nI’m also going to select() out the *_bags columns and the PLU*_sold columns, as I don’t plan to be doing any stats on the numbers of variously sized avocado bags sold per week, or avocado sales by avocado size (the different PLU values correspond to small, large, and extra large avocados). I think we can get most of the useful information in this data from the remaining columns.\n\n# Note that I'm not removing GreatLakes, NorthernNewEngland, or WestTexNewMexico\n# because I think those are counting large regions that don't have a big city\n# so they're not double-counting the other cities\navocado &lt;- avocado_raw %&gt;%\n  filter(!(region %in% c(\"California\",\n                         \"Midsouth\",\n                         \"Northeast\",\n                         \"Plains\",\n                         \"SouthCentral\",\n                         \"Southeast\",\n                         \"TotalUS\",\n                         \"West\"))) %&gt;%\n  select(-ends_with(\"bags\"), -starts_with(\"PLU\"))\n\nA latent benefit of removing unwanted columns in the data is that when we print the tibble to console to inspect the first several rows, we can see more columns that were originally pushed off to the side.\n\navocado\n\n# A tibble: 15,545 × 6\n   date       avg_price total_volume type          year region\n   &lt;date&gt;         &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;        &lt;int&gt; &lt;chr&gt; \n 1 2015-12-27      1.33       64237. conventional  2015 Albany\n 2 2015-12-20      1.35       54877. conventional  2015 Albany\n 3 2015-12-13      0.93      118220. conventional  2015 Albany\n 4 2015-12-06      1.08       78992. conventional  2015 Albany\n 5 2015-11-29      1.28       51040. conventional  2015 Albany\n 6 2015-11-22      1.26       55980. conventional  2015 Albany\n 7 2015-11-15      0.99       83454. conventional  2015 Albany\n 8 2015-11-08      0.98      109428. conventional  2015 Albany\n 9 2015-11-01      1.02       99811. conventional  2015 Albany\n10 2015-10-25      1.07       74339. conventional  2015 Albany\n# ℹ 15,535 more rows\n\n\n\nInstructor’s note: If you wish, you can set tibble printing options to print preview values for all columns of a tibble, irrespective of whether those additional columns spill over to the next row of print output. You can do it temporarily by explicitly calling print(my_tibble, width = Inf) to override the default console-width-detecting behavior, or by setting your R instance’s global options with options(tibble.width = Inf) to print all columns for all tibbles. However, I personally like the width-restricting behavior of default tibble printing. I’ll take the clean console output."
  },
  {
    "objectID": "posts/2020-04-08-tidy-multilevel/index.html#inspecting-the-data",
    "href": "posts/2020-04-08-tidy-multilevel/index.html#inspecting-the-data",
    "title": "Wrangling multilevel data in the tidyverse",
    "section": "Inspecting the data",
    "text": "Inspecting the data\nToday’s explorations will focus around the following diffuse research question: Do avocado buyers prefer buying conventional or organic avocados? By how much? How do these patterns differ from region to region? For a variety of reasons (pick your favorite), it might be interesting to try to quantify grocery shoppers’ relative preference for conventional vs. organic avocados, as it probably tracks with regional variations in supply chain access, and/or psychological attitudes about health and environmentalism.\n(We won’t be able to get into those more complex variables because they aren’t present in the avocado data we have, but just an inspiration!)\nEven before we get into looking for more complex statistics within each region, we can use simpler tidyverse tools to generate and inspect single-value summary statistics, both visually and numerically, for each metropolitan area present in the data.\n\nQuick graphs\nWe can use built-in grouping features of ggplot2 to inspect the data visually without having to pre-manipulate it much.\nFirst, let’s look at a histogram of weekly avocado sales, across all weeks, and all metropolitan areas. How many avocados are people buying in a given week? I’m also going to split this histogram to get two different filled histograms for conventional and organic avocado sales so we can see those sales separately. I’ll set the colors to… ah, avocado-themed.\n\navo_colors = c(\"sienna4\", \"olivedrab2\")\n\nplot_hist_avo_total_volume &lt;- avocado %&gt;%\n  ggplot(aes(x = total_volume, fill = type)) +\n  geom_histogram(position = \"identity\", bins = 25, alpha = 0.5) +\n  scale_fill_manual(values = avo_colors) +\n  theme_bw()\n\nplot_hist_avo_total_volume\n\n\n\n\n\n\n\n\nOoh, yikes, the weekly total avocados sold looks super gamma-skewed. Most weeks it’s maybe a few tens of thousands of avocados sold, but in some weeks and some regions it gets into the millions of avocados sold (that’s a lot of avocados in a week!), so we would probably do well to log-transform the data to try to get it to take a more normal-ish distribution.\nRight now, we can use one of ggplot2’s scale helper functions to set the scale to be on log-10 units.\n\nplot_hist_avo_total_volume +\n  scale_x_log10()\n\n\n\n\n\n\n\n\nThat looks much better! Now we can proceed.\nNext, let’s break up the graph by region, so we can see the sub-histograms of total avocados sold for each metropolitan area. A super-fast way to break up your graph by each grouping unit is to facet your ggplot, or to break it up into small multiples. I like to use facet_wrap(~ grouping_unit) to break a single plot up, so that the data for each metro area is displayed on its own sub-plot, or facet. The “wrap” part of facet_wrap() tries to arrange the facets in “reading” order, from left-to-right and then top-to-bottom to fill the plot area as efficiently as possible.\n\nplot_hist_avo_total_volume +\n  # let's not forget to log-scale the x-axis!\n  scale_x_log10() +\n  facet_wrap(~ region)\n\n\n\n\n\n\n\n\nIncidentally, graphing data at first can be a great way of identifying important variables in the data that need to be considered. For example, what if I had forgotten to break up the sales data by conventional vs. organic avocado type?\n\navocado %&gt;%\n  # Notice that I've removed the fill aesthetic\n  # so now it's not splitting by avo type\n  ggplot(aes(x = total_volume)) +\n  geom_histogram(position = \"identity\", bins = 25, alpha = 0.5) +\n  scale_x_log10() +\n  theme_bw()\n\n\n\n\n\n\n\n\nI might see a bimodal histogram and say “huh, that’s funny.” If I weren’t so careful, I might continue my business without accounting for this bimodality. But optimally I’d stop and think “what additional variable might be driving this bimodality?” and after some exploration, realize that I need to consider conventional vs. organic avocado type when looking at avocado sales in order to properly characterize the data. Always keep an eye out!\n\n\nBasic summary statistics\nIn addition to graphs, we can generate simple one-shot summary statistics using group_by() and summarize() from the dplyr package.\nAs a companion to the exploratory graphs of total_volume we just generated, we can calculate numerical summaries of the weekly number of avocados sold as a quick metric of the difference in buying patterns for conventional and organic avocados. Before we calculate these summaries, though, we should create a new column for log-10-transformed total_volume, as we saw earlier in our graphs that it’s massively right-tailed and needs to be log-transformed to look more normal.\n\n# The double-sided assignment pipe %&lt;&gt;%, from magrittr\n# you can use this by library()-ing magrittr\n# in addition to library(tidyverse)\n# avocado %&lt;&gt;% ... does the same thing as:\n# avocado &lt;- avocado %&gt;% ...\n# so it gives us a shortcut to pipe an object into some calls,\n# and then re-save the new result back into the old variable name\n# let's save total_volume_log10 into avocado because we'll need it\n# many times in the future\navocado %&lt;&gt;%\n  mutate(total_volume_log10 = log10(total_volume))\n\n\navocado %&gt;%\n  group_by(region, type) %&gt;%\n  summarize(mean_total_volume = mean(total_volume_log10),\n            sd_total_volume = sd(total_volume_log10))\n\n`summarise()` has grouped output by 'region'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 92 × 4\n# Groups:   region [46]\n   region              type         mean_total_volume sd_total_volume\n   &lt;chr&gt;               &lt;chr&gt;                    &lt;dbl&gt;           &lt;dbl&gt;\n 1 Albany              conventional              4.95          0.140 \n 2 Albany              organic                   3.29          0.205 \n 3 Atlanta             conventional              5.70          0.0899\n 4 Atlanta             organic                   4.00          0.236 \n 5 BaltimoreWashington conventional              5.88          0.0722\n 6 BaltimoreWashington organic                   4.31          0.225 \n 7 Boise               conventional              4.91          0.0987\n 8 Boise               organic                   3.34          0.209 \n 9 Boston              conventional              5.74          0.0811\n10 Boston              organic                   4.03          0.359 \n# ℹ 82 more rows\n\n\nAbove, we group_by() both region and type to get two separate means and SDs of weekly avocados sold in each region, one value each for conventional and organic avocados.\nWe can pivot_wider() our observation-level data, to get total_volume into two columns, one for the conventional avocados and one for the organic avocados, for each week. Thus, instead of two rows per week per metro region, one for each avocado type, we’ll have one row per week per metro region, with the relevant values for each avocado type in their own columns.\n\navocado %&gt;%\n  # Here, selecting the id_cols and the ONE values_from column I want to pivot out\n  select(year, date, region, type, total_volume_log10) %&gt;%\n  pivot_wider(names_from = type,\n              values_from = total_volume_log10,\n              names_prefix = \"total_volume_\")\n\n# A tibble: 7,774 × 5\n    year date       region total_volume_conventional total_volume_organic\n   &lt;int&gt; &lt;date&gt;     &lt;chr&gt;                      &lt;dbl&gt;                &lt;dbl&gt;\n 1  2015 2015-12-27 Albany                      4.81                 3.00\n 2  2015 2015-12-20 Albany                      4.74                 3.07\n 3  2015 2015-12-13 Albany                      5.07                 3.00\n 4  2015 2015-12-06 Albany                      4.90                 3.06\n 5  2015 2015-11-29 Albany                      4.71                 2.92\n 6  2015 2015-11-22 Albany                      4.75                 2.93\n 7  2015 2015-11-15 Albany                      4.92                 3.08\n 8  2015 2015-11-08 Albany                      5.04                 3.12\n 9  2015 2015-11-01 Albany                      5.00                 3.01\n10  2015 2015-10-25 Albany                      4.87                 3.07\n# ℹ 7,764 more rows\n\n\n\nInstructor’s note: Above, I’ve select()-ed to remove all the other outcome columns other than total_volume before pivoting my dataframe. If you wanted to pivot all outcome columns in the data by type, you could, as of tidyr &gt;=1.0.0. The least verbose way to do this would be to feed a negative column name vector into the values_from argument to exclude ID columns from the set of columns to pivot, like so:\n\n\navocado %&gt;%\n  pivot_wider(names_from = type, values_from = -c(year, date, region))\n\n# A tibble: 7,774 × 11\n   date        year region avg_price_conventional avg_price_organic\n   &lt;date&gt;     &lt;int&gt; &lt;chr&gt;                   &lt;dbl&gt;             &lt;dbl&gt;\n 1 2015-12-27  2015 Albany                   1.33              1.83\n 2 2015-12-20  2015 Albany                   1.35              1.89\n 3 2015-12-13  2015 Albany                   0.93              1.85\n 4 2015-12-06  2015 Albany                   1.08              1.84\n 5 2015-11-29  2015 Albany                   1.28              1.94\n 6 2015-11-22  2015 Albany                   1.26              1.94\n 7 2015-11-15  2015 Albany                   0.99              1.89\n 8 2015-11-08  2015 Albany                   0.98              1.88\n 9 2015-11-01  2015 Albany                   1.02              1.88\n10 2015-10-25  2015 Albany                   1.07              1.83\n# ℹ 7,764 more rows\n# ℹ 6 more variables: total_volume_conventional &lt;dbl&gt;,\n#   total_volume_organic &lt;dbl&gt;, type_conventional &lt;chr&gt;, type_organic &lt;chr&gt;,\n#   total_volume_log10_conventional &lt;dbl&gt;, total_volume_log10_organic &lt;dbl&gt;\n\n\n\nThen, by default, since there are multiple values_from columns, each of them has the relevant level of names_from (here, avocado type) appended to the end of the column name with the names_sep separator.\n\nOnce the data is pivoted, we can now use mutate() to subtract the organic total dollars spent from the conventional total dollars spent. Then, we can generate similar mean/SD summaries as we did before. Finally, we can make the little tibble print preview a bit more informative by using arrange() to sort the data such that the metro areas with highest conventional - organic sales volume differences get sorted to the top of the tibble.\n\navocado %&gt;%\n  select(year, date, region, type, total_volume_log10) %&gt;%\n  pivot_wider(names_from = type,\n              values_from = total_volume_log10,\n              names_prefix = \"total_volume_\") %&gt;%\n  mutate(total_volume_diff = total_volume_conventional - total_volume_organic) %&gt;%\n  group_by(region) %&gt;%\n  summarize(mean_volume_diff = mean(total_volume_diff),\n            sd_volume_diff = sd(total_volume_diff)) %&gt;%\n  arrange(desc(mean_volume_diff))\n\n# A tibble: 46 × 3\n   region            mean_volume_diff sd_volume_diff\n   &lt;chr&gt;                        &lt;dbl&gt;          &lt;dbl&gt;\n 1 MiamiFtLauderdale             2.25          0.285\n 2 Tampa                         2.01          0.228\n 3 PhoenixTucson                 1.99          0.134\n 4 NewOrleansMobile              1.91          0.247\n 5 GrandRapids                   1.84          0.390\n 6 Orlando                       1.84          0.170\n 7 DallasFtWorth                 1.80          0.177\n 8 Houston                       1.76          0.235\n 9 Sacramento                    1.76          0.149\n10 Jacksonville                  1.74          0.171\n# ℹ 36 more rows\n\n\nHuh, cities in Florida appear to be over-represented in the top several metro areas in buying more conventional than organic avocados. I wonder what’s going on there? We can’t rule out that this is driven by Florida having waaaay cheaper conventional avocados, and relatively more expensive organic avocados, which could be driving Floridian shoppers to choose conventional. We probably want to take this into account in later analyses, so that any effects of conventional vs. organic we see on the volume of avocados sold aren’t driven by a huge price premium for organic avocados. (Stay tuned!)\nWe can get a fair amount of mileage out of these initial visualizations and summaries, and there’s nothing wrong with using these summaries as a first pass. I do so all the time! However, when you want (or need) to be able to calculate more complex statistics on the weekly data for each metro area, you’ll need more complex tools."
  },
  {
    "objectID": "posts/2020-04-08-tidy-multilevel/index.html#nesting-the-data",
    "href": "posts/2020-04-08-tidy-multilevel/index.html#nesting-the-data",
    "title": "Wrangling multilevel data in the tidyverse",
    "section": "Nesting the data",
    "text": "Nesting the data\nNearly all of the tidyverse features we’ll be working through in the rest of this tutorial rely on two key features of R to make their magic happen: list-columns and flexible vectorizing functions.\n\nIntro to nest(), unnest(), and tibble list-columns\nA df is composed of columns, each of which is a vector of identical length. But nobody ever said the columns of a df all have to be atomic vectors! A df column can be a list vector, where each element of said list-column can contain objects of different data types and lengths. While each element of list-column can contain objects of various and sundry types/lengths, list-columns gain a lot of power when they contain objects of consistent type. (When this is true, you can apply a single function to every row of a list-column, and get a single column of consistent output–this is where we’re headed.)\nIn this way, a list-column allows you to store multiple observations per unit wrapped up in such a way that the main df still has one row per unit, but you retain the observation-level data because you haven’t actually collapsed across any variables.\nFirst, we’ll create a new variable by folding our observation-level data into a new shape, using nest() from the tidyr package. This is going to package the long-form avocado data into nested form, which is somewhere between long data (one row per observation) and wide data (one row per grouping unit).\n\navo_by_region &lt;- avocado %&gt;%\n  nest(sales = -region)\n\nA nested dataframe like the one we just created has two main column types: key columns and list-columns of sub-dataframes.\nKey columns are the columns you might group_by() before, say, generating summary statistics with summarize(). They act as a label for the data contained in that row’s list-columns. List-columns do not contain a single atomic element in each row, but instead _a whole sub-data frame containing all the observation-level data corresponding to that key level.\n\nInstructor’s note: A deeper exploration of the uses for and behaviors of list-columns is beyond the scope of the current tutorial. This tutorial assumes learners have never intentionally worked with non-dataframe list objects, and demonstrates only the case of list-columns containing data nested from an original “normal” dataframe. R for Data Science’s section on dataframe list-columns is a useful reference for more information on list-columns.\n\nNow, for list-columns, instead of actually printing the content of the vector, tibble output gives us a blurb about the object contained inside of each list element.\n\navo_by_region\n\n# A tibble: 46 × 2\n   region              sales             \n   &lt;chr&gt;               &lt;list&gt;            \n 1 Albany              &lt;tibble [338 × 6]&gt;\n 2 Atlanta             &lt;tibble [338 × 6]&gt;\n 3 BaltimoreWashington &lt;tibble [338 × 6]&gt;\n 4 Boise               &lt;tibble [338 × 6]&gt;\n 5 Boston              &lt;tibble [338 × 6]&gt;\n 6 BuffaloRochester    &lt;tibble [338 × 6]&gt;\n 7 Charlotte           &lt;tibble [338 × 6]&gt;\n 8 Chicago             &lt;tibble [338 × 6]&gt;\n 9 CincinnatiDayton    &lt;tibble [338 × 6]&gt;\n10 Columbus            &lt;tibble [338 × 6]&gt;\n# ℹ 36 more rows\n\n\nWe can see here that each row of sales, the list-column generated by nesting the data, contains a sub-tibble with 338 rows of the other 6 columns of data corresponding to each level of region. The data is still there, safe and sound!\nnest() creates list-columns with the general argument format list_col = cols_to_put_into_list_col. Importantly, the columns you wish to put into the list-column can be specified using select() syntax. We can harness this to use the - to exclude specific key columns, and put all other non-key columns into the list-column. For example, if you wanted to hold out multiple key columns, for example, both region and type to split up the sub-data by conventional and organic:\n\navocado %&gt;%\n  nest(sales = -c(region, type))\n\n# A tibble: 92 × 3\n   type         region              sales             \n   &lt;chr&gt;        &lt;chr&gt;               &lt;list&gt;            \n 1 conventional Albany              &lt;tibble [169 × 5]&gt;\n 2 conventional Atlanta             &lt;tibble [169 × 5]&gt;\n 3 conventional BaltimoreWashington &lt;tibble [169 × 5]&gt;\n 4 conventional Boise               &lt;tibble [169 × 5]&gt;\n 5 conventional Boston              &lt;tibble [169 × 5]&gt;\n 6 conventional BuffaloRochester    &lt;tibble [169 × 5]&gt;\n 7 conventional Charlotte           &lt;tibble [169 × 5]&gt;\n 8 conventional Chicago             &lt;tibble [169 × 5]&gt;\n 9 conventional CincinnatiDayton    &lt;tibble [169 × 5]&gt;\n10 conventional Columbus            &lt;tibble [169 × 5]&gt;\n# ℹ 82 more rows\n\n\nIn this way, you can flexibly choose which columns of data to fold into your list-column, and which key-columns you want to fold by.\nWhen you want to unnest the data and go back to original long form, you might not be surprised to hear that the function we’ll need is unnest().\n\navo_by_region %&gt;%\n  unnest(sales)\n\n# A tibble: 15,545 × 7\n   region date       avg_price total_volume type         year total_volume_log10\n   &lt;chr&gt;  &lt;date&gt;         &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;       &lt;int&gt;              &lt;dbl&gt;\n 1 Albany 2015-12-27      1.33       64237. convention…  2015               4.81\n 2 Albany 2015-12-20      1.35       54877. convention…  2015               4.74\n 3 Albany 2015-12-13      0.93      118220. convention…  2015               5.07\n 4 Albany 2015-12-06      1.08       78992. convention…  2015               4.90\n 5 Albany 2015-11-29      1.28       51040. convention…  2015               4.71\n 6 Albany 2015-11-22      1.26       55980. convention…  2015               4.75\n 7 Albany 2015-11-15      0.99       83454. convention…  2015               4.92\n 8 Albany 2015-11-08      0.98      109428. convention…  2015               5.04\n 9 Albany 2015-11-01      1.02       99811. convention…  2015               5.00\n10 Albany 2015-10-25      1.07       74339. convention…  2015               4.87\n# ℹ 15,535 more rows\n\n\nunnest() takes as its argument the list-column you wish to unnest, and then thy will be done, it takes the data back into long form for you. Notice that the key column, in this case region, is now once again repeated for every row in the data belonging to that key level.\nTo inspect the contents of a single element of a list-column, we have to index into the list-column. The tidyverse function to index single columns, equivalent to dollar-sign $ indexing dataframe columns in base R, is pull(). pull() is pipe-safe, meaning it takes a tibble/dataframe as its first argument, so we can pipe said dataframe into the function. Then, the second argument that we specify inside of the parentheses is the column name we want to index, “naked”, without quotation marks, as with other tidyverse functions.\n\navo_by_region %&gt;%\n  pull(region)\n\n [1] \"Albany\"              \"Atlanta\"             \"BaltimoreWashington\"\n [4] \"Boise\"               \"Boston\"              \"BuffaloRochester\"   \n [7] \"Charlotte\"           \"Chicago\"             \"CincinnatiDayton\"   \n[10] \"Columbus\"            \"DallasFtWorth\"       \"Denver\"             \n[13] \"Detroit\"             \"GrandRapids\"         \"GreatLakes\"         \n[16] \"HarrisburgScranton\"  \"HartfordSpringfield\" \"Houston\"            \n[19] \"Indianapolis\"        \"Jacksonville\"        \"LasVegas\"           \n[22] \"LosAngeles\"          \"Louisville\"          \"MiamiFtLauderdale\"  \n[25] \"Nashville\"           \"NewOrleansMobile\"    \"NewYork\"            \n[28] \"NorthernNewEngland\"  \"Orlando\"             \"Philadelphia\"       \n[31] \"PhoenixTucson\"       \"Pittsburgh\"          \"Portland\"           \n[34] \"RaleighGreensboro\"   \"RichmondNorfolk\"     \"Roanoke\"            \n[37] \"Sacramento\"          \"SanDiego\"            \"SanFrancisco\"       \n[40] \"Seattle\"             \"SouthCarolina\"       \"Spokane\"            \n[43] \"StLouis\"             \"Syracuse\"            \"Tampa\"              \n[46] \"WestTexNewMexico\"   \n\n\nWe can see below that base R $ indexing yields the same result:\n\navo_by_region$region\n\n [1] \"Albany\"              \"Atlanta\"             \"BaltimoreWashington\"\n [4] \"Boise\"               \"Boston\"              \"BuffaloRochester\"   \n [7] \"Charlotte\"           \"Chicago\"             \"CincinnatiDayton\"   \n[10] \"Columbus\"            \"DallasFtWorth\"       \"Denver\"             \n[13] \"Detroit\"             \"GrandRapids\"         \"GreatLakes\"         \n[16] \"HarrisburgScranton\"  \"HartfordSpringfield\" \"Houston\"            \n[19] \"Indianapolis\"        \"Jacksonville\"        \"LasVegas\"           \n[22] \"LosAngeles\"          \"Louisville\"          \"MiamiFtLauderdale\"  \n[25] \"Nashville\"           \"NewOrleansMobile\"    \"NewYork\"            \n[28] \"NorthernNewEngland\"  \"Orlando\"             \"Philadelphia\"       \n[31] \"PhoenixTucson\"       \"Pittsburgh\"          \"Portland\"           \n[34] \"RaleighGreensboro\"   \"RichmondNorfolk\"     \"Roanoke\"            \n[37] \"Sacramento\"          \"SanDiego\"            \"SanFrancisco\"       \n[40] \"Seattle\"             \"SouthCarolina\"       \"Spokane\"            \n[43] \"StLouis\"             \"Syracuse\"            \"Tampa\"              \n[46] \"WestTexNewMexico\"   \n\n\nThere are definitely still instances where $ indexing columns is the cleanest way to index. However, in this tutorial we’ll be using pipe-safe tidyverse indexing functions moving forward so that we can stick with the %&gt;% pipe for consistency.\nOnce we’ve used pull() to index a single column, we can use another tidyverse pipe-safe helper function, pluck(), this time for indexing elements of a vector. As a rule of thumb, pull() does what $ indexing does, and pluck() does what [] indexing does. Thus, the below code indexes the sales column, and then specifically the first element in that column vector.\n\navo_by_region %&gt;%\n  pull(sales) %&gt;%\n  pluck(1)\n\n# A tibble: 338 × 6\n   date       avg_price total_volume type          year total_volume_log10\n   &lt;date&gt;         &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;        &lt;int&gt;              &lt;dbl&gt;\n 1 2015-12-27      1.33       64237. conventional  2015               4.81\n 2 2015-12-20      1.35       54877. conventional  2015               4.74\n 3 2015-12-13      0.93      118220. conventional  2015               5.07\n 4 2015-12-06      1.08       78992. conventional  2015               4.90\n 5 2015-11-29      1.28       51040. conventional  2015               4.71\n 6 2015-11-22      1.26       55980. conventional  2015               4.75\n 7 2015-11-15      0.99       83454. conventional  2015               4.92\n 8 2015-11-08      0.98      109428. conventional  2015               5.04\n 9 2015-11-01      1.02       99811. conventional  2015               5.00\n10 2015-10-25      1.07       74339. conventional  2015               4.87\n# ℹ 328 more rows\n\n\nWe can also use the dplyr tools we already know to subset avo_by_region() to pluck() the data belonging to a particular region.\n\navo_by_region %&gt;%\n  # This should give us a one-row dataframe\n  # so we know the first element is the one we want\n  filter(region == \"NewYork\") %&gt;%\n  pull(sales) %&gt;%\n  pluck(1)\n\n# A tibble: 338 × 6\n   date       avg_price total_volume type          year total_volume_log10\n   &lt;date&gt;         &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;        &lt;int&gt;              &lt;dbl&gt;\n 1 2015-12-27      1.17     1129876. conventional  2015               6.05\n 2 2015-12-20      1.23     1139348. conventional  2015               6.06\n 3 2015-12-13      1.12     1254805. conventional  2015               6.10\n 4 2015-12-06      1.2      1068972. conventional  2015               6.03\n 5 2015-11-29      1.16      999170. conventional  2015               6.00\n 6 2015-11-22      1.14     1111803. conventional  2015               6.05\n 7 2015-11-15      1.04     1357393. conventional  2015               6.13\n 8 2015-11-08      1.13     1406262. conventional  2015               6.15\n 9 2015-11-01      1.06     2180520. conventional  2015               6.34\n10 2015-10-25      1.23     1048046. conventional  2015               6.02\n# ℹ 328 more rows\n\n\nNotice that once you’ve got only one row in a nested dataframe, pluck()-ing to unlist the data in the list-column is equivalent to using unnest() to unfold all rows of the list-column.\n\navo_by_region %&gt;%\n  filter(region == \"NewYork\") %&gt;%\n  unnest(sales)\n\n# A tibble: 338 × 7\n   region  date       avg_price total_volume type        year total_volume_log10\n   &lt;chr&gt;   &lt;date&gt;         &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;      &lt;int&gt;              &lt;dbl&gt;\n 1 NewYork 2015-12-27      1.17     1129876. conventio…  2015               6.05\n 2 NewYork 2015-12-20      1.23     1139348. conventio…  2015               6.06\n 3 NewYork 2015-12-13      1.12     1254805. conventio…  2015               6.10\n 4 NewYork 2015-12-06      1.2      1068972. conventio…  2015               6.03\n 5 NewYork 2015-11-29      1.16      999170. conventio…  2015               6.00\n 6 NewYork 2015-11-22      1.14     1111803. conventio…  2015               6.05\n 7 NewYork 2015-11-15      1.04     1357393. conventio…  2015               6.13\n 8 NewYork 2015-11-08      1.13     1406262. conventio…  2015               6.15\n 9 NewYork 2015-11-01      1.06     2180520. conventio…  2015               6.34\n10 NewYork 2015-10-25      1.23     1048046. conventio…  2015               6.02\n# ℹ 328 more rows\n\n\n\nInstructor’s note: To use exclusively base R to index into list-columns, you have to be careful when you want to use single-bracket indexing (e.g. to conditional-index based in values in another column of the df) versus double-bracket indexing (to actually expose the value of the list-column for printing to console or otherwise manipulation). For a more in-depth exploration of base R indexing into lists, please visit the relevant R for Data Science chapter."
  },
  {
    "objectID": "posts/2020-04-08-tidy-multilevel/index.html#going-vectorize-crazy",
    "href": "posts/2020-04-08-tidy-multilevel/index.html#going-vectorize-crazy",
    "title": "Wrangling multilevel data in the tidyverse",
    "section": "Going vectorize crazy",
    "text": "Going vectorize crazy\nNext, we’ll review vectorizing: this is a feature of R that you probably use all the time and don’t even know it! We’ll learn a little more about what vectorized functions do and how you can vectorize any function you wish.\nTo read more in-depth about vectorized functions in R, check out Noam Ross’s handy blog post. We won’t be getting deep into the nuts and bolts of how vectorization works in R today. We’ll just be discussing it at a high level that should orient you enough to be able to make quick use of vectorizing helper functions.\n\nVectorizing: it’s like a for loop, but not\nAs you’ve likely experienced, much data processing requires writing code to perform a particular action, and then wrapping it in other code to repeat that action many times over many objects. In R, this usually means doing the same operation to every element in a vector or every row in a df column.\nIn most computing languages, the most sensible way to do one thing a bunch of times is to use a for loop, which (I know you know this but I’ll spell it out just to be extra) repeats the code inside the loop once for every element of a vector that’s specified at the beginning of the loop code.\nTo use a for loop to apply some function to every element of a df column, you might write something like:\n\nfor (i in 1:nrow(df)) {\n  df$new_col[i] = my_function(df$old_col[i])\n}\n\nand that would do the trick. In this case, the vector you iterate along contains the row indices of your df, which means that on every iteration of your loop you’re calling my_function() on the next row of data until you get to the end of your df.\nThis is a perfectly functional way to write code! Today, I’ll argue that it’s prone to typos that might cause you big headaches. Specifically, these typos may cause your code to fail silently (ominous organ riff), or do something unintended without throwing an error. This means you wouldn’t find out that something was wrong unless you visually inspected the output, which you can’t always do after every command (I getcha). Thus, there’s a risk of inducing mistakes in the data that you don’t catch until it’s too late.\nFor example, once I wrote a for loop just like the above, to perform some functions on every row of a df column, but I wrote df$old_col[1] instead of df$old_col[i]. I accidentally produced the same value in every element of df$new_col, because even though the for loop was iterating as usual, on every iteration of the loop it was calling the same value. I didn’t discover my typo until weeks later. No bueno!\nEnter… vectorizing.\nIt turns out that R has built-in optimized functionality for repeating functions along every element of a vector, just waiting for you to harness!\nGenerally, any vectorized function:\n\ntakes a vector as input\ndoes the same thing to every element of that vector\nreturns a vector of the same length as output\n\nPlenty of functions are already vectorized, and you likely already use them as such, including:\n\nmath operators (R assumes element-wise math as the default, and element-wise == vectorized here)\n\narithmetic operators: +, -, and the like\nround(), for example\n\nstring manipulation functions\n\npaste() always returns a vector equal to the longest input vector\npattern matching functions like grep(), grepl(), and such\n\nstatistical distribution functions\n\nthe d*(), p*(), and q*() statistical distribution functions (e.g. qnorm()) are all vectorized along their main input arguments\n\nother stuff too!\n\nbase::ifelse() is vectorized, as it determines true/false for each pair of elements in the two vector arguments element-wise\nand MANY more\n\n\nIn general, it’s worth always checking to see if the function you plan to call on a vector is vectorized, and then you can use it on your vector or dataframe column without needing any additional looping.\n\n\nTidy vectorizing\nIf you use mutate() for your everyday column-wise data manipulation needs, using vectorized functions is smooth. In fact, mutate() is built to encourage you to use vectorized functions for column manipulation.\nIf we go back to the Hass avocado sale data, we can see this at work. For example, let’s take another look at the code we used earlier to create a new column for the weekly total number of avocados sold, but log-10 transformed. Earlier, we saw order-of-magnitude differences in avocado sales between different regions, so we log-transformed the data to get it to be more normally distributed.\n\navocado %&gt;%\n  mutate(total_volume_log10 = log10(total_volume)) %&gt;%\n  select(region, date, total_volume, total_volume_log10)\n\n# A tibble: 15,545 × 4\n   region date       total_volume total_volume_log10\n   &lt;chr&gt;  &lt;date&gt;            &lt;dbl&gt;              &lt;dbl&gt;\n 1 Albany 2015-12-27       64237.               4.81\n 2 Albany 2015-12-20       54877.               4.74\n 3 Albany 2015-12-13      118220.               5.07\n 4 Albany 2015-12-06       78992.               4.90\n 5 Albany 2015-11-29       51040.               4.71\n 6 Albany 2015-11-22       55980.               4.75\n 7 Albany 2015-11-15       83454.               4.92\n 8 Albany 2015-11-08      109428.               5.04\n 9 Albany 2015-11-01       99811.               5.00\n10 Albany 2015-10-25       74339.               4.87\n# ℹ 15,535 more rows\n\n\nWe can write this in one call, without needing to wrap it in a loop, because log10() is vectorized, and returns a vector the same length as the input vector. Neat! You can use any vectorized functions “naked” (without any extra looping code) to create new dataframe columns inside of mutate() with no problem.\nBut what about functions that aren’t already vectorized? Can we use R magic to make them vectorized, if we so wish? Why yes, we can!\n\nIntro to map(): Anything is vectorized if you want\nAt their root, vectorizing helper functions do very nearly what a for loop does, but with a little more specificity in their syntax that can help protect you from tricky typos. With a vectorizing function, you still specify the two main code components that you’d specify in a for loop:\n\nThe vector you want to iterate along\nThe code you want to run on every element of that vector\n\nThe nice thing about using a vectorizer function is that it’s designed to return a vector of the same length as the input vector, so it’s perfect for data manipulation on columns in a dataframe. For loops are more flexible in that they can run any code a bunch of times, and don’t necessarily need to return a vector. If your goal is to return an output vector of the same length as your input vector, then vectorizer functions can do the trick with less work!\nThe basic tidyverse vectorizer function is purrr::map(). (The purrr package is so named because its helper functions can make your code purr with contentment, hah.) It takes the two arguments you’d expect: an input vector, and the code to run on each element of the input vector.\nThe main situation where vectorizer functions come in really handy is to vectorize functions to run along every element of a non-atomic list vector. R’s default vectorized functions are all intended for atomic vectors, and not necessarily list vectors. To vectorize arguments along a list, we need a little help.\nTo illustrate this briefly, we’ll use map() to vectorize an operation that isn’t vectorized by default and run it on every element of a list. We can use pull() to index the sales list-column of our nested avocado data. Indexing this list-column gives us …just… a list, where each element is a dataframe. We can then use map() to compute a single operation for every value in the list. Here, since every element in the list is a dataframe, we can use nrow() to count the number of rows of every list element.\n\navo_by_region %&gt;%\n  # Here, I'm using slice() to get only the first 10 rows of the tibble\n  # to shorten the eventual output. Otherwise it would output the nrows\n  # for every single region in the avocado data\n  slice(1:10) %&gt;%\n  pull(sales) %&gt;%\n  map(\\(x) nrow(x))\n\n[[1]]\n[1] 338\n\n[[2]]\n[1] 338\n\n[[3]]\n[1] 338\n\n[[4]]\n[1] 338\n\n[[5]]\n[1] 338\n\n[[6]]\n[1] 338\n\n[[7]]\n[1] 338\n\n[[8]]\n[1] 338\n\n[[9]]\n[1] 338\n\n[[10]]\n[1] 338\n\n\nAha! We see the same numbers of rows per sub-dataframe that we saw earlier in R’s automatic tibble output. (Notice that this output looks a little different than a normal atomic vector, what with the [[]] double bracket indices and all… we’ll address this very soon.)\nmap(), like the vast majority of tidyverse functions, is pipe-safe, so the first argument is the vector to be iterated along. (Notice that the first argument is expected to be a vector, not an entire dataframe, but otherwise pipe-safe.)\nThe second argument clearly looks like the code that you want run on each element of that vector, but the syntax is a little different than if you were to just call nrow(avocado$sales[[1]]) or something. This particular syntax is how tidyverse functions allow you to specify functions inside other functions without confusing R about what code to run where. Let’s unpack this a little bit:\nThe code you put in the second argument is preceded with \\(x), which (as of R &gt;= 4.0) is a shortcut to define an anonymous function. An anonymous function is kind of like a “naked” function, where its underlying is showing without being assigned to a function name. Because the function is defined with no name, it can’t be reused without copying and pasting the code. But the upside is that you can define a quick little function to do one small thing without needing to store it in a function name!\nThe variable x that you see inside of nrow(x) is the anonymous function argument that was previously named using \\(x). (If you’d done \\(y), the argument would need to be called nrow(y) instead!) In this case, the anonymous function argument is a placeholder that tells map() how to feed your input vector into your looping code. Whenever you see that x inside of map() code, you can refer it as “the element I’m currently working with” in your mental pseudo-code. In this case, you can interpret the map() call as doing the following:\n\nTake the list sales, which is a column in avo_by_region\nWith every element in that list:\n\nCalculate the number of rows of the dataframe held inside that list element\n\nReturn another list where the nth element contains the nrows of the nth element of sales\n\nIn this way, you can see how similar the pseudo-code of map() is to the pseudo-code of a for loop. Not so intimidating, hopefully!\n\nInstructor’s note: Since December 2022, the purrr maintainers now recommend using \\(x) anonymous function shorthand over formula syntax inside of map(). Tilde ~ formula-style syntax still works, as does calling a bare function names as the object, but I recommend teaching anonymous function shorthand only for non-advanced learners. Avoiding confusion is more important than a comprehensive explanation at this level.\n\n\n\nSpecial versions of map()\nIt’s important to remember this: map() always returns a list. Even if each element of the output list contains an element of the same atomic type, and length 1, map() is not going to try to guess anything and turn your output into an atomic vector. The philosophy of the tidyverse is to be conservative, and avoid guessing data types of output, lest it be wrong.\nIf you, the coder, know exactly the data type you expect in your output vector, you can use a specialized version of map() that will return an atomic output vector of your desired data type, or else throw an error (at which point you would go and fix the offending code). These are all called map_*(), where the suffix indicates the expected data type of your output.\nFor example, in the code chunk we used earlier where we used map() to calculate nrow() for every sub-dataframe of the sales column of avo_by_region(), every single value of nrow() was a number. More specifically, each one was an integer (makes sense, you can only have a integer number of rows in a dataframe), so if we use map_int() in place of map(), R will automatically unwrap our output into an atomic integer vector.\n\navo_by_region %&gt;%\n  slice(1:10) %&gt;%\n  pull(sales) %&gt;%\n  map_int(\\(x) nrow(x))\n\n [1] 338 338 338 338 338 338 338 338 338 338\n\n\nThere’s a version of map_*() for every possible atomic vector data type.\nAdded benefit: these functions will also will not coerce any output data types without your knowledge. If you pick the wrong map_*() function for your output data type, or the code inside map_*() doesn’t actually return the data type you thought it would, you’ll get an R error that should remind you to go back and fix the offending code.\n\n\nUsing map() on list-columns inside a df\nRemember from before when we learned about creating nested dataframes, that nesting creates a list-column where each list element contains another, smaller dataframe pertaining to just the data from that row’s record. What if we wanted to manipulate the data contained inside each sub-dataframe? Since the column of nested data is a list, we should be able to use map() to operate on it.\nHere, we’ll see how we can use map() to calculate subject-level summary statistics without discarding observation-level data from the dataframe.\nFirst, let’s re-nest our observation-level avocado data, but this time, we’ll do it by region and by avocado type (conventional vs organic).\n\navo_by_region_type &lt;- avocado %&gt;%\n  # Notice the -c() select() syntax to exclude TWO key columns\n  # from the list-column\n  nest(sales = -c(region, type))\n\nNow, to see what the new, nested df looks like:\n\navo_by_region_type\n\n# A tibble: 92 × 3\n   type         region              sales             \n   &lt;chr&gt;        &lt;chr&gt;               &lt;list&gt;            \n 1 conventional Albany              &lt;tibble [169 × 5]&gt;\n 2 conventional Atlanta             &lt;tibble [169 × 5]&gt;\n 3 conventional BaltimoreWashington &lt;tibble [169 × 5]&gt;\n 4 conventional Boise               &lt;tibble [169 × 5]&gt;\n 5 conventional Boston              &lt;tibble [169 × 5]&gt;\n 6 conventional BuffaloRochester    &lt;tibble [169 × 5]&gt;\n 7 conventional Charlotte           &lt;tibble [169 × 5]&gt;\n 8 conventional Chicago             &lt;tibble [169 × 5]&gt;\n 9 conventional CincinnatiDayton    &lt;tibble [169 × 5]&gt;\n10 conventional Columbus            &lt;tibble [169 × 5]&gt;\n# ℹ 82 more rows\n\n\nAnd to verify that the observation-level sales data for the first region and avocado type is safe and sound in the first row of the list-column sales:\n\navo_by_region_type %&gt;%\n  pull(sales) %&gt;%\n  pluck(1)\n\n# A tibble: 169 × 5\n   date       avg_price total_volume  year total_volume_log10\n   &lt;date&gt;         &lt;dbl&gt;        &lt;dbl&gt; &lt;int&gt;              &lt;dbl&gt;\n 1 2015-12-27      1.33       64237.  2015               4.81\n 2 2015-12-20      1.35       54877.  2015               4.74\n 3 2015-12-13      0.93      118220.  2015               5.07\n 4 2015-12-06      1.08       78992.  2015               4.90\n 5 2015-11-29      1.28       51040.  2015               4.71\n 6 2015-11-22      1.26       55980.  2015               4.75\n 7 2015-11-15      0.99       83454.  2015               4.92\n 8 2015-11-08      0.98      109428.  2015               5.04\n 9 2015-11-01      1.02       99811.  2015               5.00\n10 2015-10-25      1.07       74339.  2015               4.87\n# ℹ 159 more rows\n\n\nNow, because our nested df has one row per region x type, it’s perfectly set up to contain some additional columns with summary statistics in them. We just need to be able to access the observation-level data inside of the sales column in order to compute these summary stats.\nWe can use mutate() to create a new column in our dataframe as per usual. However, this time, we’ll call map() inside of mutate() to create our own vectorized function that can operate on a list-column.\nFor our first summary statistic, let’s calculate the average avocado price across all weeks for each metropolitan area, separately for the two avocado types. We’ll do this by using map() to create a function that will call mean() on every element in the sales list-column, and we’ll wrap all of this inside mutate() so we can work inside of our main nested dataframe.\nBelow is the full function call, so you can see the output first:\n\navo_by_region_type %&gt;%\n  mutate(overall_avg_price = map_dbl(sales, \\(x) x %&gt;%\n                                       pull(avg_price) %&gt;%\n                                       mean()\n                                     )\n         )\n\n# A tibble: 92 × 4\n   type         region              sales              overall_avg_price\n   &lt;chr&gt;        &lt;chr&gt;               &lt;list&gt;                         &lt;dbl&gt;\n 1 conventional Albany              &lt;tibble [169 × 5]&gt;              1.35\n 2 conventional Atlanta             &lt;tibble [169 × 5]&gt;              1.07\n 3 conventional BaltimoreWashington &lt;tibble [169 × 5]&gt;              1.34\n 4 conventional Boise               &lt;tibble [169 × 5]&gt;              1.08\n 5 conventional Boston              &lt;tibble [169 × 5]&gt;              1.30\n 6 conventional BuffaloRochester    &lt;tibble [169 × 5]&gt;              1.38\n 7 conventional Charlotte           &lt;tibble [169 × 5]&gt;              1.28\n 8 conventional Chicago             &lt;tibble [169 × 5]&gt;              1.37\n 9 conventional CincinnatiDayton    &lt;tibble [169 × 5]&gt;              1.02\n10 conventional Columbus            &lt;tibble [169 × 5]&gt;              1.07\n# ℹ 82 more rows\n\n\nNow to unpack the pieces of this function call:\n\nInside of mutate(), we follow the same usual syntax of new_col = function(old_col), but this time the function we call is map()\nThe first argument of map() is the column whose elements we wish to iterate over, in this case sales\nThe second argument of map() is what we wish to do to each element of sales\n\nthe function call is preceded by a tilde ~ per map()’s expected syntax\nwe first call .x, which refers to whatever’s contained in each element of sales. In this case, .x refers to a dataframe\nwe then use %&gt;% to pipe .x into the next operation we wish to use on it, as you would use the pipe in normal usage outside of map()\nwe pipe into pull() to choose only the column avg_price that’s contained inside each sub-dataframe of the sales list-column\nwe pipe the avg_price column into mean() to take the mean of that column\n\nWe actually call map_dbl() instead of just map() because we expect each vector element to be a double with length 1, and so we use map_dbl() to safely unwrap the output vector into an atomic double vector\n\nYou can use the same general strategy to calculate additional summary statistics, each with their own column created by using map() or one of its atomic output cousins on a list-column of dataframes.\n\navo_by_region_type %&gt;%\n  # I'm indenting the close parentheses and such this way\n  # to use the indentation levels to visually cue\n  # which chunk of code lives in which level\n  # because we are pretty deep here!\n  mutate(overall_avg_price = map_dbl(sales,\n                                     \\(x) x %&gt;%\n                                       pull(avg_price) %&gt;%\n                                       mean()\n                                     ),\n         avg_total_volume = map_dbl(sales,\n                                    \\(x) x %&gt;%\n                                      pull(total_volume) %&gt;%\n                                      mean()\n                                    )\n         )\n\n# A tibble: 92 × 5\n   type         region              sales    overall_avg_price avg_total_volume\n   &lt;chr&gt;        &lt;chr&gt;               &lt;list&gt;               &lt;dbl&gt;            &lt;dbl&gt;\n 1 conventional Albany              &lt;tibble&gt;              1.35           92903.\n 2 conventional Atlanta             &lt;tibble&gt;              1.07          512789.\n 3 conventional BaltimoreWashington &lt;tibble&gt;              1.34          773642.\n 4 conventional Boise               &lt;tibble&gt;              1.08           82843.\n 5 conventional Boston              &lt;tibble&gt;              1.30          561541.\n 6 conventional BuffaloRochester    &lt;tibble&gt;              1.38          130410.\n 7 conventional Charlotte           &lt;tibble&gt;              1.28          203331.\n 8 conventional Chicago             &lt;tibble&gt;              1.37          759816.\n 9 conventional CincinnatiDayton    &lt;tibble&gt;              1.02          247835.\n10 conventional Columbus            &lt;tibble&gt;              1.07          169268.\n# ℹ 82 more rows\n\n\nThis will get you to roughly the same place as using group_by() and summarize() to calculate summary statistics on an unnested dataframe, and saving those summary stats into a second dataframe. The reason I like the list-column-plus-summaries setup is that I can keep all of my information in a single master dataframe and avoid proliferation of objects across my workspace. Keeping the data organized in a master dataframe also helps you avoid pitfalls like accidentally changing the order of rows in one of your dataframes with arrange(), then breaking an assumption in later code that’s based on the row index of a particular piece of data (that’s now pulling from different data than you thought!). If you keep your data yoked together as columns in a single dataframe, it saves you from misaligning your data with wayward row sorting.\nUltimately, it’s a matter of personal taste, but if you find that you like managing your observation-level data and your unit-level summary data in this way, then you can use these techniques as you like!\n\n\n\nA caveat: When not to vectorize\nUltimately, I love vectorizing functions like map()–they’ve helped me keep a lot more of my data processing inside of tidyverse functions, and often saved me from copying and pasting code. There are, of course, cases when writing your code to work inside of vectorizing helpers isn’t necessarily optimal. Here are a couple that I’ve run into:\n\nIf you need to recursively reference earlier vector elements while looping\n\nVectorizer functions run every iteration of your vectorizing loop independently, which means that the nth iteration cannot access the output of the (n-1)th iteration or any previous ones. If you need to be able to access previous loop content in later iterations, a for loop is the way to go.\n\nIf you really need a progress bar to print to console, for iterating over looooong vectors\n\nTrue console-based progress bars with must recursively access the console to create a progress bar that stays on one line from 0% to 100%, so they work best inside for loops\nThe good folks at RStudio who maintain purrr are still working on building automatic progress bars for map(), but as of publishing they’re not implemented\nCheck out the progress package, which contains helper functions that work inside for loops to make your own progress bars"
  },
  {
    "objectID": "posts/2020-04-08-tidy-multilevel/index.html#hella-models",
    "href": "posts/2020-04-08-tidy-multilevel/index.html#hella-models",
    "title": "Wrangling multilevel data in the tidyverse",
    "section": "Hella models",
    "text": "Hella models\nNow that we’ve demonstrated its functionality through a simpler example, we can get into the the real star of using map() inside mutate() to work with list-columns in a dataframe. If each element of a list can contain any R object that could otherwise be stored in its own variable, then a list should be able to hold model objects in it. And we can create each model object in that list-column of models by using map() to fit a model to every sub-dataframe in a list-column of dataframes. Then, if we produce a list-column where each element is a model object with the same values in it, because it’s the same model fit to different data, we can use map() again to extract model coefficients from each model in the list into their own list-column. In this way, we can generate more complex (and hopefully more informative) statistics about each of the grouping units in our multilevel data.\nLet’s get back to the question driving our earlier data exploration, before we did a dive into list-columns and map(): Do avocado buyers prefer buying conventional or organic avocados? By how much? How do these patterns differ from region to region?\nThe most obvious number that might be informative is the difference in total_volume, the total number of avocados sold in any given week, for type == \"organic\" vs type == \"conventional\". If people buy more conventional avocados than organic avocados, we can assume they prefer the conventional avocados. But we know that the organic avocados are generally more expensive than conventional avocados in any given week. We can look at a plot of weekly prices, with the prices split for the two avocado types, to see that the price line for organic avocados is almost always more expensive.\n\navocado %&gt;%\n  ggplot(aes(x = date, y = avg_price, color = type)) +\n  geom_line() +\n  scale_color_manual(values = avo_colors) +\n  facet_wrap(~ region) +\n  labs(y = \"Average price (dollars per avocado)\",\n       title = \"Weekly prices per avocado, by organic vs. conventional\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nGrocery shoppers might be less likely to buy as many conventional avocados at all just because of the price. How can we take into account that organic avocados are more expensive when estimating the difference in volume of avocados purchased, to make our analysis more robust?\nIf we can adjust for the usual premium price on organic avocados, we can estimate the difference in total volume of avocados purchased, over and above that which you would expect when one type of avocado is cheaper. That might be able to tell us something like the following: if conventional and organic avocados were the same price, what is the estimated difference in volume of avocados purchased?\nThis sounds like a job for linear regression!\nIn the most basic case, we might consider doing a paired t-test between the weekly volume of conventional vs. organic avocados sold, and that could tell us whether those weekly volumes are “different”, but this t-test would be less informative than an equivalent linear regression (which estimates the same difference). Further, the linear regression allows us to create a multiple regression that lets us estimate that difference in volume of avocados sold, but adjusted for another variable–in this case, the average avocado price.\nThe model formula would look something like this:\ntotal_volume ~ type + avg_price\nWe’ll want to use lm() to fit this model to each region separately, so that we can see the variation in avocado buying preference across each metropolitan area. An incidental benefit of running a multiple regression that adjusts for avg_price is that it helps adjust for differences in avocado prices across regions that are likely due to differences in cost of living from place to place.\n\nFitting many models\nWe can use dataframe list-columns, manipulated with map() inside mutate(), to fit this particular model to the avocado sales data from every metropolitan area. Since mutate() is amenable to defining a flexible number of columns, we can first do some light data preprocessing on the observation-level data contained in the sales column, and then in the next command, within mutate(), create a new list-column using map() to fit an lm() model to each row’s data in sales.\n\n# Again, the double-sided pipe in magrittr\n# to overwrite the old value of avo_by_region with the new value\n# with all the new columns in it\navo_by_region %&lt;&gt;%\n  mutate(sales = map(sales,\n                     # Yes that's right folks, we are about to use\n                     # mutate() inside map() inside mutate()\n                     \\(x) x %&gt;%\n                       # Here, let's roughly re-center avg_price\n                       # so the \"baseline\", or 0 point, is set at $1.00/avocado\n                       # we already calculate total_volume_log10 earlier\n                       # so we should be good there\n                       mutate(avg_price_shift1 = avg_price - 1)\n                     ),\n         # Now that we've preprocesed our columns\n         # we can fit our multiple regression\n         # using lm() inside of map()\n         model = map(sales,\n                     \\(x) lm(total_volume_log10 ~ type + avg_price_shift1, data = x)\n                     )\n         )\n\n\nInstructor’s note: You might be tempted to mean-center and perhaps z-score the avg_price column, so that it’s in units of standard deviations of avocado prices. However, you need to be careful how your data is grouped/nested when you center or z-score data, to know whether you’re centering your data within grouping unit or across grouping units. That is, are you centering/scaling by a mean/SD calculated separately for each grouping unit, or for all observations across units? In some cases, centering/scaling data within unit can be dangerous, because you might accidentally be scaling away the effect of interest. Scaling data across units retains between-unit differences of interest, so it’s fine in this case. You would need to unnest your observation-level data to make sure your operations are not implicitly grouped before doing any mean-centering/SD-scaling operations.\n\nNow that we’ve created the model column to hold each of our lm objects, let’s quickly see what the whole dataframe looks like:\n\navo_by_region\n\n# A tibble: 46 × 3\n   region              sales              model \n   &lt;chr&gt;               &lt;list&gt;             &lt;list&gt;\n 1 Albany              &lt;tibble [338 × 7]&gt; &lt;lm&gt;  \n 2 Atlanta             &lt;tibble [338 × 7]&gt; &lt;lm&gt;  \n 3 BaltimoreWashington &lt;tibble [338 × 7]&gt; &lt;lm&gt;  \n 4 Boise               &lt;tibble [338 × 7]&gt; &lt;lm&gt;  \n 5 Boston              &lt;tibble [338 × 7]&gt; &lt;lm&gt;  \n 6 BuffaloRochester    &lt;tibble [338 × 7]&gt; &lt;lm&gt;  \n 7 Charlotte           &lt;tibble [338 × 7]&gt; &lt;lm&gt;  \n 8 Chicago             &lt;tibble [338 × 7]&gt; &lt;lm&gt;  \n 9 CincinnatiDayton    &lt;tibble [338 × 7]&gt; &lt;lm&gt;  \n10 Columbus            &lt;tibble [338 × 7]&gt; &lt;lm&gt;  \n# ℹ 36 more rows\n\n\nAha! We see that in the list-column model, each of the top rows contains an lm object. Each of these lm objects individually can be operated on like any linear regression object stored in its own variable.\n\navo_by_region %&gt;%\n  pull(model) %&gt;%\n  pluck(1)\n\n\nCall:\nlm(formula = total_volume_log10 ~ type + avg_price_shift1, data = x)\n\nCoefficients:\n     (Intercept)       typeorganic  avg_price_shift1  \n          5.0298           -1.5590           -0.2388  \n\n\nWe can even %&gt;% this further into summary() to generate the whole regression output table:\n\navo_by_region %&gt;%\n  pull(model) %&gt;%\n  pluck(1) %&gt;%\n  summary()\n\n\nCall:\nlm(formula = total_volume_log10 ~ type + avg_price_shift1, data = x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.42005 -0.10962  0.00827  0.10662  0.51076 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       5.02979    0.02068 243.168  &lt; 2e-16 ***\ntypeorganic      -1.55898    0.02687 -58.023  &lt; 2e-16 ***\navg_price_shift1 -0.23883    0.04608  -5.183 3.78e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1693 on 335 degrees of freedom\nMultiple R-squared:  0.9605,    Adjusted R-squared:  0.9603 \nF-statistic:  4077 on 2 and 335 DF,  p-value: &lt; 2.2e-16\n\n\nWhile this is not in spirit a multiple regression tutorial, we are doing a multiple regression, so now’s a handy time to review interpreting multiple regression coefficients:\n\nIntercept: This tells us the estimated log-10-transformed volume of conventional avocados sold in a hypothetical week where conventional avocados cost $1.00 each\n\nBy default the type is dummy-coded so the baseline level is the first alphabetical level, and C comes before O, so the intercept assumes the baseline level is conventional avocados. Fine by me!\nThis is why we created avg_price_shift1 earlier. If we hadn’t shifted the 0 point of avg_price, the intercept would be estimated for a week where conventional avocados cost $0.00 each. I’m assuming avocados are never free (otherwise we millenials would be able to afford mortgages), so shifting the baseline can make the regression coefficient estimates easier to interpret.\n\ntypeorganic: This tells us the estimated log-10-transformed difference in volume of avocados sold, between conventional and organic avocados, in a hypothetical week where both avocado types cost $1.00 each\navg_price_shift1: This tells us the expected difference in volume of avocados sold between a week where avocados cost $1.00 each and a week where avocados are one price unit, or one dollar, more expensive ($2.00 each)\n\nImportantly, because this is a multiple regression, the typeorganic term tells us the estimate difference in volume of avocados sold with price “held constant”, or if both conventional and organic avocados cost the same in a hypothetical week. If we can estimate the difference volume sold when the two avocado types cost the same, and people are STILL buying more conventional avocados, there must be an extenuating reason why people don’t buy organic avocados even if they’re not more expensive.\nThe console output that you get from calling summary() on a single lm object is great for visual inspection, but there’s a whole lot of output here that is not the most efficient to parse programmatically with code. As such, we won’t be using map() inside mutate() to call summary() on every single model object in the model column. There are other functions we can use to give us cleaner, more ready-to-use output, that we’ll explore next.\n\n\nExtracting statistics from models\nIn order to programmatically extract coefficients, standard errors, and other statistics of interest from each model object in the model column of avo_by_region, our nested avocado sale data, we are going to use the tidy() helper function in the broom R package. broom is designed for generating tidyverse-safe versions of statistical analysis objects to get you from models to plots & interpretation with as little hacking as possible.\nFirst, to demonstrate the output of broom::tidy(), we can call it just on the first model element of the model column of avo_by_region just as we called summary() on it above.\n\navo_by_region %&gt;%\n  pull(model) %&gt;%\n  pluck(1) %&gt;%\n  # The only thing that's different is here,\n  # the last command we call after we've plucked a single lm object:\n  broom::tidy()\n\n# A tibble: 3 × 5\n  term             estimate std.error statistic   p.value\n  &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)         5.03     0.0207    243.   0        \n2 typeorganic        -1.56     0.0269    -58.0  7.89e-177\n3 avg_price_shift1   -0.239    0.0461     -5.18 3.78e-  7\n\n\nNote that I am using the :: to call tidy() directly out of the broom package namespace without library()-ing the package. I prefer to use this syntax when I’m only using a single function from a package in a particular analysis document, to avoid loading too many function names into my session. (sometimes functions in different packages have overlapping names so you want to be careful!)\nCalling tidy() on a single lm object gives us a tibble, with a column for each statistic of interest in the regression table, and a row for each term from the multiple regression. This has the same numbers as the core table output we got from calling summary() on the same object, but now with all the added benefits of a tibble. We can use select() to choose specific columns of statistics, filter() to choose specific rows corresponding to particular coefficients, mutate() to calculate new statistics from the ones we already have, and all that good stuff!\nAnother huge benefit of tidy() returning a tibble of model output for each model is that if we use tidy() inside map() inside mutate() to create a list-column of coefficients tibbles, we can create a list-column of tibble dataframes, which has specific tidyverse abilities.\n\navo_by_region %&lt;&gt;%\n  mutate(coefs = map(model, \\(x) broom::tidy(x)))\n\nThe biggest tidyverse ability is that the resulting coefs list-column of coefficients dataframes can then be pulled out into long form with unnest().\n\n# Storing this long dataframe into its own object\n# so we can feed it into ggplot2 calls more quickly later!\navo_coefs &lt;- avo_by_region %&gt;%\n  # See the note below for why we're using select()\n  # to retain only these two columns\n  select(region, coefs) %&gt;%\n  unnest(coefs)\n\navo_coefs\n\n# A tibble: 138 × 6\n   region              term             estimate std.error statistic   p.value\n   &lt;chr&gt;               &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 Albany              (Intercept)         5.03     0.0207    243.   0        \n 2 Albany              typeorganic        -1.56     0.0269    -58.0  7.89e-177\n 3 Albany              avg_price_shift1   -0.239    0.0461     -5.18 3.78e-  7\n 4 Atlanta             (Intercept)         5.72     0.0128    446.   0        \n 5 Atlanta             typeorganic        -1.57     0.0243    -64.8  1.67e-191\n 6 Atlanta             avg_price_shift1   -0.240    0.0305     -7.88 4.64e- 14\n 7 BaltimoreWashington (Intercept)         5.94     0.0182    327.   0        \n 8 BaltimoreWashington typeorganic        -1.51     0.0229    -65.9  6.99e-194\n 9 BaltimoreWashington avg_price_shift1   -0.165    0.0383     -4.31 2.14e-  5\n10 Boise               (Intercept)         4.93     0.0107    460.   0        \n# ℹ 128 more rows\n\n\nAnd here we are, a long dataframe with a row for each coefficient from the multiple regression, and a chunk of rows for the coefficients from the model fit to a particular metro area. Usefully, this long dataframe can then be fed into a ggplot2 plotting call, to visualize all the model outputs from each region together. That’s what we’ll get to next!\n\nNotice that as of tidyr &gt;=1.0.0, unnest()’s default behavior is to retain any other list-columns present in the df that aren’t getting unnested. If you unnested a list-column of dataframes that each had multiple rows, e.g. the coefs column, where each sub-dataframe has a row for each coefficient, the other list-column(s) getting retained would have their values repeated for each of the new rows created by unnesting the other column. Sometimes you might want that, but sometimes you might not want to increase your df’s memory footprint so much. In those cases, you should use select() to remove list-columns that you don’t want repeated before unnesting.\n\n\n\nVisualizing many statistics\nNow that we have our model coefficients in a long form dataframe, with a chunk of rows for each metropolitan area, let’s plot the model estimates for each region!\nHere, we’ll focus on plotting the values of the typeorganic term, as this will index each region’s relative buying preference (or anti-preference?) for organic relative to conventional avocados. This estimate will conveniently also be adjusted for average avocado price across regions, so it should not be affected by general differences in avocado prices (and cost of living) from region to region.\n\navo_coefplot &lt;- avo_coefs %&gt;%\n  filter(term == \"typeorganic\") %&gt;%\n  # Here, using fct_reorder() from the forcats pkg of the tidyverse\n  # which provides a bunch of nice factor manipulating functions\n  # super DUPER handy for reordering factor levels on a plot\n  # this reorders the regions in order from highest to lowest estimate\n  ggplot(aes(x = fct_reorder(region, desc(estimate)), y = estimate)) +\n  # geom_pointrange takes x and y, like geom_point, \n  # AND ymin/ymax (for vertical bars) or xmin/xmax (for horizontal bars) \n  # to plot a point/errorbar combo\n  geom_pointrange(aes(ymin = estimate - std.error,\n                      ymax = estimate + std.error),\n                  width = 0) +\n  # let's annotate\n  # to add guides to remind us how to interpret the log-10 scale\n  # that stuff is brain-bendy!\n  geom_hline(yintercept = c(-1, -2), linetype = 3) +\n  annotate(\"text\", x = 8, y = -1.05, hjust = 0,\n           label = \"-1 on log10 = 10% as many\", color = \"black\") +\n  annotate(\"text\", x = 8, y = -1.95, hjust = 0,\n           label = \"-2 on log10 = 1% as many\", color = \"black\") +\n  # since the region names are long and would otherwise overlap on the plot,\n  # tilting them to 45 degrees will make them all stick off the plot on an angle\n  # the new helper function guide_axis() takes care of specifics\n  # when called within scale_x_discrete() as below (since we have discrete labels)\n  # (as of ggplot2 v3.3.0)\n  scale_x_discrete(guide = guide_axis(angle = 45)) +\n  labs(x = \"Metro area\",\n       y = \"Log-10 diff in volume of organic avos sold\",\n       title = \"People really do not buy organic avocados\",\n       subtitle = \"especially not people in Florida\") +\n  theme_bw()\n\nWarning in geom_pointrange(aes(ymin = estimate - std.error, ymax = estimate + :\nIgnoring unknown parameters: `width`\n\navo_coefplot\n\n\n\n\n\n\n\n\nOn this plot, metro areas on the left of the plot are areas that buy a greater percentage of organic avocados relative to conventional, and metro areas on the right of the plot are areas that buy a lower percentage of avocados relative to conventional.\nBe mindful of the log scale when reading these numbers off the plot–thinking on the log scale can be tricky! Since we’re using log-10 scaling here, a log-10 difference of -1 corresponds to a real difference of 10-1, or a 10% reduction in number. Similarly, a log-10 difference of -2 corresponds to a real difference of 10-2, or a 1% reduction in number. (A log-10 difference of -1.5 is not as intuitive, we’re all better off just calculating 101.5)\nSo in Seattle, we estimate that when the price premium for organic avocados is taken into account, people buy approximately 10% as many organic avocados as they do conventional avocados. Meanwhile, in Miami & Fort Lauderdale, we estimate that people buy less than 1% as many organic avocados as they do conventional avocados. A log-10 difference of 1 is a whole order of magnitude different in raw avocado units!\n\nInstructor’s note: The following plot takes a bit of fiddling to set up, and requires chaining together many pieces of data wrangling logic. If learners have never made a model-estimate-plus-raw-data plot like this before, this may be a bit beyond scope to live-code. In that case, I recommend taking a couple minutes for learners to brainstorm possible strategies for extending the basic coefficient plot shown above, and then showing a fully rendered copy of the plot below as an example extension plot, qualitatively describing the added layers and annotations.\n\n\navo_raw_diffs &lt;- avocado %&gt;%\n  # first, prepare to calculate the raw difference in weekly volume\n  # of organic - conventional avocados bought\n  # (must be that direction bc conventional is baseline in the regression)\n  select(region, date, type, total_volume_log10) %&gt;%\n  pivot_wider(names_from = type,\n              values_from = total_volume_log10) %&gt;%\n  mutate(diff_raw = organic - conventional) %&gt;%\n  # WestTextNewMexico has some weeks with no organic avocado sales\n  # so we need to filter out some NA rows of diff_raw\n  filter(!is.na(diff_raw)) %&gt;%\n  # for each region, calculate the mean and SE of the mean\n  # of this difference\n  group_by(region) %&gt;%\n  summarize(diff_mean = mean(diff_raw),\n            diff_se = sd(diff_raw)/sqrt(length(diff_raw))) %&gt;% \n  # in order for the plot to work\n  # (for the regions to be ordered by the model estimate)\n  # the model estimates need to be bound on to the raw diffs\n  left_join(avo_coefs %&gt;% filter(term == \"typeorganic\"),\n            by = \"region\")\n\navo_coefplot +\n  # error bars and points for the raw data\n  # notice that we use the data argument to feed in different data\n  geom_pointrange(aes(y = diff_mean,\n                      ymin = diff_mean - diff_se,\n                      ymax = diff_mean + diff_se),\n                  data = avo_raw_diffs,\n                  color = \"olivedrab4\",\n                  width = 0) +\n  # hand-adding a legend\n  annotate(\"text\", x = 45, y = -1.2, hjust = 1,\n           label = \"black = model-estimated\", color = \"black\") +\n  annotate(\"text\", x = 45, y = -1.3, hjust = 1,\n           label = \"green = raw\", color = \"olivedrab4\")\n\nWarning in geom_pointrange(aes(y = diff_mean, ymin = diff_mean - diff_se, :\nIgnoring unknown parameters: `width`\n\n\n\n\n\n\n\n\n\nI think we’ve gotten pretty close to an answer (not THE answer, but AN answer) to our earlier research question: Do avocado buyers prefer buying conventional or organic avocados? By how much? How do these patterns differ from region to region?\nAvocado buyers across the US appear to prefer buying conventional avocados over organic avocados; that is, they buy many more conventional avocados than organic avocados in a given week. This holds even when you adjust for the fact that organic avocados are usually more expensive. The magnitude of this buying preference against organic avocados varies across the country–the Pacific Northwest appears to avoid organic avocados much less than the rest of the country, and Florida appears to avoid organic avocados much more than the rest of the country.\nThat concludes the core analysis of this tutorial! If you like, continue following along for a rapid-fire demonstration of another use case of map() inside mutate() with nested dataframes to repeat analysis across many datasets containing similarly structured data."
  },
  {
    "objectID": "posts/2020-04-08-tidy-multilevel/index.html#bonus-example-bootstrapping",
    "href": "posts/2020-04-08-tidy-multilevel/index.html#bonus-example-bootstrapping",
    "title": "Wrangling multilevel data in the tidyverse",
    "section": "Bonus example: bootstrapping",
    "text": "Bonus example: bootstrapping\n\nInstructor’s note: This section is designed as a demonstration of features rather than a live how-to. The main body of the code-along lesson concludes after generating a summary graph of model statistics for each region. The lesson teaches one example case of using map() inside mutate() to fit a model to a bunch of sub-dataframes in a nested master dataframe, but there are other analysis contexts where this strategy comes in handy.\n\nAnother situation where you might fit the same model to a bunch of different datasets is bootstrapping. When bootstrapping a statistic, you would:\n\nresample your observations with replacement to yield a resampled dataset with the same N as your original dataset\ndo that resampling for many iterations to yield many resampled datasets\ndo your original analysis on every resampled dataset\nextract the statistic of interest\nuse the distribution of that statistic across your bootstrapping iterations as the error distribution around the original value of the statistic calculated on the raw data\n\nFitting many models and extracting key statistics using map() inside mutate() lends itself well to bootstrapping, which requires repeating the modeling operation of interest over many resampled datasets. If you had a nested dataframe, with a list-column of resampled datasets, you could absolutely use map() inside mutate() to fit a model to each resampled dataset, extract statistics, and get those statistics back into a long-form dataframe with a row for each bootstrap iteration, which you could then visualize with, say, a histogram or density plot to examine the distribution.\nThe one new technique I’ll demonstrate here is the bootstraps function from the rsample package in the extended tidyverse. rsample contains functions for sampling data for things like cross-validation and holding out testing data to test model predictions. In our case, we’ll be using a single function, bootstraps(), which as you might guess generates bootstrap resamples of a dataset. bootstraps() and other sampling functions in rsample have the added benefit of using special tools under the hood to reduce the size of resampled datasets in memory, which helps reduce stress on your R session.\nIn this demonstration, I’ll bootstrap the error distribution on New York’s model estimate for the avocado volume ~ type + price multiple regression we ran earlier. First, I’ll pull out New York’s raw data, and then feed it into bootstraps() to create a dataframe with a row for every bootstrap iteration, and a list-column containing that iteration’s resampled dataset.\n\navo_nyc_boot &lt;- avo_by_region %&gt;%\n  filter(region == \"NewYork\") %&gt;%\n  # Mustn't forget to select() only the group id column\n  # and the list-column we wish to unnest\n  # lest we accidentally replicate all the extra columns\n  # in this case, we want the column containing the RAW data\n  select(region, sales) %&gt;%\n  # unnesting will give us a long dataframe with just New York's weekly data\n  unnest(sales) %&gt;%\n  # this WHOLE dataframe get fed in as the first argument of bootstraps\n  # the times argument sets the # of bootstrap iterations\n  rsample::bootstraps(times = 200)\n\nLet’s see what the dataframe output of bootstraps() looks like:\n\navo_nyc_boot\n\n# Bootstrap sampling \n# A tibble: 200 × 2\n   splits            id          \n   &lt;list&gt;            &lt;chr&gt;       \n 1 &lt;split [338/122]&gt; Bootstrap001\n 2 &lt;split [338/118]&gt; Bootstrap002\n 3 &lt;split [338/122]&gt; Bootstrap003\n 4 &lt;split [338/124]&gt; Bootstrap004\n 5 &lt;split [338/137]&gt; Bootstrap005\n 6 &lt;split [338/126]&gt; Bootstrap006\n 7 &lt;split [338/126]&gt; Bootstrap007\n 8 &lt;split [338/138]&gt; Bootstrap008\n 9 &lt;split [338/119]&gt; Bootstrap009\n10 &lt;split [338/125]&gt; Bootstrap010\n# ℹ 190 more rows\n\n\nbootstraps() returns a dataframe with a character column called id which labels each iteration, and a list column called splits where each element houses metadata for a resampled dataset. I say metadata because part of the trick that bootstraps() uses is that the native split object type doesn’t actually contain any data on its own:\n\navo_nyc_boot %&gt;%\n  pull(splits) %&gt;%\n  pluck(1)\n\n&lt;Analysis/Assess/Total&gt;\n&lt;338/122/338&gt;\n\n\nThese numbers aren’t really designed for human consumption on their own. The real magic happens when you call as.data.frame() on a split object, which triggers it to actually do the resampling and give you a tibble of data.\n\navo_nyc_boot %&gt;%\n  pull(splits) %&gt;%\n  pluck(1) %&gt;%\n  as.data.frame()\n\n# A tibble: 338 × 8\n   region  date       avg_price total_volume type        year total_volume_log10\n   &lt;chr&gt;   &lt;date&gt;         &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;      &lt;int&gt;              &lt;dbl&gt;\n 1 NewYork 2017-08-20      1.75     1057189. conventio…  2017               6.02\n 2 NewYork 2018-03-18      1.7       189434. organic     2018               5.28\n 3 NewYork 2017-05-28      2.38       76002. organic     2017               4.88\n 4 NewYork 2015-08-02      2.12       13852. organic     2015               4.14\n 5 NewYork 2016-08-21      2.16       30583. organic     2016               4.49\n 6 NewYork 2016-09-04      1.47     1278749. conventio…  2016               6.11\n 7 NewYork 2017-08-20      1.85      107275. organic     2017               5.03\n 8 NewYork 2016-12-11      2.22       32659. organic     2016               4.51\n 9 NewYork 2015-02-01      1.36     1433763. conventio…  2015               6.16\n10 NewYork 2016-07-31      2.41       24048. organic     2016               4.38\n# ℹ 328 more rows\n# ℹ 1 more variable: avg_price_shift1 &lt;dbl&gt;\n\n\nTo double-check that the data were actually resampled with replacement, we can look in this first resampled tibble to make sure that at least some of the rows appear multiple times in the data.\n\navo_nyc_boot %&gt;%\n  pull(splits) %&gt;%\n  pluck(1) %&gt;%\n  as.data.frame() %&gt;%\n  count(date) %&gt;%\n  arrange(desc(n))\n\n# A tibble: 144 × 2\n   date           n\n   &lt;date&gt;     &lt;int&gt;\n 1 2016-12-18     6\n 2 2015-05-24     5\n 3 2015-12-27     5\n 4 2016-06-19     5\n 5 2016-09-18     5\n 6 2016-12-11     5\n 7 2017-05-14     5\n 8 2017-05-28     5\n 9 2015-02-22     4\n10 2015-04-19     4\n# ℹ 134 more rows\n\n\nLooks resampled to me! Now, we can use the same strategy we used before, but instead of operating on the raw data for each region contained in the sales column of avo_by_region, we’ll operate on the resampled data for each bootstrap id contained in the splits column of avo_nyc_boot.\n\navo_nyc_boot %&lt;&gt;%\n  # Note that here, I'm not saving the models into their own column\n  # but instead piping the model result of lm()\n  # DIRECTLY into broom::tidy() to go directly to the coefficients tibble\n  mutate(coefs_boot = map(splits,\n                          \\(x) x %&gt;%\n                            # we need to call this first\n                            # to turn the splits object into data \n                            as.data.frame() %&gt;%\n                            # now we may proceed as usual\n                            lm(total_volume_log10 ~ type + avg_price_shift1, data = .) %&gt;%\n                            broom::tidy()\n                          )\n         ) %&gt;%\n  # again, use select to remove any columns we don't want repeated\n  # when unnesting\n  select(-splits) %&gt;%\n  unnest(coefs_boot)\n\nIf we inspect the long-form unnested content of avo_nyc_boot after we’ve pulled out the bootstrapped coefficients, it looks pretty similar to the unnested coefficients in avo_coefs, but this time instead of coefficients by region, we’ve got coefficients by bootstrap id.\n\navo_nyc_boot\n\n# A tibble: 600 × 6\n   id           term             estimate std.error statistic   p.value\n   &lt;chr&gt;        &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 Bootstrap001 (Intercept)         6.23     0.0285    218.   0        \n 2 Bootstrap001 typeorganic        -1.41     0.0450    -31.3  2.03e-101\n 3 Bootstrap001 avg_price_shift1   -0.255    0.0562     -4.54 7.99e-  6\n 4 Bootstrap002 (Intercept)         6.28     0.0283    222.   0        \n 5 Bootstrap002 typeorganic        -1.31     0.0441    -29.7  7.27e- 96\n 6 Bootstrap002 avg_price_shift1   -0.383    0.0564     -6.78 5.51e- 11\n 7 Bootstrap003 (Intercept)         6.29     0.0289    217.   0        \n 8 Bootstrap003 typeorganic        -1.28     0.0409    -31.4  1.13e-101\n 9 Bootstrap003 avg_price_shift1   -0.397    0.0539     -7.37 1.35e- 12\n10 Bootstrap004 (Intercept)         6.24     0.0279    223.   0        \n# ℹ 590 more rows\n\n\nThese coefficients are now ready to feed into a ggplot2 histogram. I’ll just inspect the bootstrapped error distribution for typeorganic, the coefficient for the weekly difference in (log-10) volume of organic avocados sold relative to conventional avocados.\n\navo_nyc_boot %&gt;%\n  filter(term == \"typeorganic\") %&gt;%\n  ggplot(aes(x = estimate)) +\n  geom_histogram() +\n  theme_bw()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nWe can see that this error distribution shows high kurtosis; that is, that the tails are short and the center of distribution is thick. This means we probably don’t want to use the standard deviation of this distribution as the standard error of the estimate, since the error distribution isn’t normal. Instead, we can use percentiles to report error intervals non-parametrically.\nFor the last plot I’ll show in this demo, I’ll do a bit more data manipulation to join the bootstrapped standard error data and the “raw” lm()-estimated standard error data for the original New York avocado sales model, to produce a plot comparing the standard errors from the model estimate and from bootstrapping. This way we can see how much more conservative (how much larger) the bootstrapped standard error is relative to the lm()-estimated error.\n\navo_nyc_boot %&gt;%\n  # for each term in the model, over all bootstrap iterations,\n  group_by(term) %&gt;%\n  # calculate its mean, SD,\n  # and the 16th and 84th percentiles\n  # to correspond with estimate +- 1 SE\n  # bc percentiles are non-parametric\n  # so, safe for non-normal distribution\n  summarize(estimate_boot = mean(estimate),\n            se_boot = sd(estimate),\n            q16_boot = quantile(estimate, .16),\n            q84_boot = quantile(estimate, .84)) %&gt;%\n  # now, horizontally join on the coefs from the raw data\n  left_join(avo_coefs %&gt;%\n              filter(region == \"NewYork\") %&gt;%\n              select(term, estimate_raw = estimate, se_raw = std.error) %&gt;%\n              # assuming a normal distribution,\n              # rough-calculate the expected 16th and 84th percentiles\n              # to go along with the values in the bootstrapped coefs\n              mutate(q16_raw = estimate_raw - se_raw,\n                     q84_raw = estimate_raw + se_raw),\n            by = \"term\") %&gt;%\n  # use pivot_longer to get all the cols other than term,\n  # which are duplicated for the boot and raw coefs,\n  # into long form with a column indicating boot vs. raw\n  pivot_longer(cols = -term,\n               names_to = c(\".value\", \"model_type\"),\n               # see the pivot_longer docs for more info on this argument!\n               names_pattern = \"(.*)_(.*)\") %&gt;%\n  # again, just plotting the coefs for typeorganic\n  filter(term == \"typeorganic\") %&gt;%\n  ggplot(aes(x = estimate, y = model_type)) +\n  geom_pointrange(aes(xmin = q16,\n                      xmax = q84),\n                  height = 0) +\n  labs(x = \"Estimate +- 1 'standard error'\",\n       y = \"error type\",\n       title = \"Bootstrap vs. 'raw' errors for the New York avocado sale volume regression\",\n       subtitle = \"The bootstrapped error is more conservative\") +\n  theme_bw()\n\nWarning in geom_pointrange(aes(xmin = q16, xmax = q84), height = 0): Ignoring\nunknown parameters: `height`\n\n\n\n\n\n\n\n\n\nYep, it looks like the error bounds on the bootstrapped data are wider than the equivalent standard error from the lm fit to the raw data. This will usually be the case! Perhaps a good argument to use bootstrapped standard errors more often, to be conservative with our statistics."
  },
  {
    "objectID": "posts/2024-05-21-multiple-r-versions.html",
    "href": "posts/2024-05-21-multiple-r-versions.html",
    "title": "How to install and switch between multiple R versions on a Mac",
    "section": "",
    "text": "Every time I start a new analysis project, I like to try to start it on the newest available version of R so I can benefit from all those nice updates. (Additionally, some packages update their minimum compatible R version so not only do I get the newest version of R, I also get it to play most nicely with the newest version of packages!)\nHowever, R’s default installation behavior when you use a .pkg installer on Mac is to uninstall all other installed versions of R. Which is quite bad for reproducibility! If my analysis code pipelines are basically independent from one another, I ought to be able to have multiple versions of R installed at the same time and associate different versions of R with different project folders.\nTurns out, the framework (or the Framework… you’ll see) is already there! With a few additional steps, you too can become the master of versions. (And then you’ll have no excuse for starting your new R projects using that old, dusty R version…)"
  },
  {
    "objectID": "posts/2024-05-21-multiple-r-versions.html#its-pretty-easy-on-linux-actually",
    "href": "posts/2024-05-21-multiple-r-versions.html#its-pretty-easy-on-linux-actually",
    "title": "How to install and switch between multiple R versions on a Mac",
    "section": "It’s pretty easy on Linux, actually",
    "text": "It’s pretty easy on Linux, actually\nIf you’re running on Linux, the default R installer behavior is to install R without uninstalling existing versions. How nice! As such, you should be able to install whichever additional versions you want without following any special instructions. Then follow the Linux section of these official Posit instructions to change which version RStudio attempts to open."
  },
  {
    "objectID": "posts/2024-05-21-multiple-r-versions.html#its-also-pretty-easy-on-windows",
    "href": "posts/2024-05-21-multiple-r-versions.html#its-also-pretty-easy-on-windows",
    "title": "How to install and switch between multiple R versions on a Mac",
    "section": "It’s also pretty easy on Windows",
    "text": "It’s also pretty easy on Windows\nIf you’re running on Windows, old forum posts suggest that the default R installer behavior is also to install R without uninstalling existing versions. (Why does it only do it for Mac, then…?) According to the official Posit instructions linked above, you can hold down the Ctrl key when clicking the RStudio icon and a dialog box will appear asking you to choose which version of R to open RStudio with."
  },
  {
    "objectID": "posts/2024-05-21-multiple-r-versions.html#meanwhile-for-macs",
    "href": "posts/2024-05-21-multiple-r-versions.html#meanwhile-for-macs",
    "title": "How to install and switch between multiple R versions on a Mac",
    "section": "Meanwhile, for Macs…",
    "text": "Meanwhile, for Macs…\nBelow, see the steps for maintaining and switching between simultaneously installed versions of R on a Mac.\nThese instructions were inspired by Jacob Price’s blog post, but updated given R/Mac OS changes over time.\n\nMake your Mac “forget” that R is already installed\nAs I mentioned before, the evil Mac .pkg installer for R will by default uninstall any other installed versions of R before installing the new one. It does this by checking through the Mac’s list of installed application packages and removing all folders listed as being installed with R.\nHowever, when the installer installs the files, it actually by default installs R into a MAJOR.MINOR version-specific subfolder of the R install folder. That means that, for example, R 4.3.2 is installed into a folder for R 4.3. R 4.4.0 would get installed into a folder for R 4.4, which does not require overwriting R 4.3.2.\n\nNote: You cannot have multiple simultaneous versions of R with the same major and minor version but different patch numbers. For example, R 4.3.2 and R 4.0.0 simultaneously are okay, but R 4.3.2 and R 4.3.3 will overwrite each other’s files.\n\nThus, the only thing you need to do to trick your Mac into not deleting other R versions is to remove those R application files from the list of installed packages, without actually deleting the application files themselves.\nThis way, when the new R installer runs, it doesn’t think there is any old version of R to uninstall.\nFirst, in a terminal, use the pkgutil command to list all of your Mac’s “known” app packages associated with R.\n\npkgutil --pkgs='org.?-project*'\n\norg.R-project.R.GUI.pkg\norg.r-project.x86_64.texinfo\norg.R-project.R.fw.pkg\norg.r-project.x86_64.tcltk\n\n\nThe --pkgs flag can take a regex string as shown above. This regex will find all packages that start with org.r-project or org.R-project. Yeah, the capital/lowercase R thing is annoying. Be careful!\nYou’ll probably get the same package list as I did, but defer to what shows up on your own terminal (for example, if you’re running an Intel Mac instead of an Apple chip Mac.)\nEach of these files pertains to a different component of R’s underlying application package source, including the GUI, the source code, and associated LaTeX info. Now, once for each of the file names you see, run, for example:\nsudo pkgutil --forget org.R-project.arm64.R.GUI.pkg\nYou need to do this once for each of the files that are listed when you run pkgutil --pkgs='org.?-project*'. You can make sure you’ve gotten them all by checking that no packages show up when you run that command.\nNow, your Mac thinks R is no longer installed. But when you run:\n\n# These are the R versions that I have installed\nls -lh /Library/Frameworks/R.framework/Versions\n\ntotal 0\ndrwxrwxr-x  6 root    admin   192B Sep 13 16:35 4.0\ndrwxrwxr-x  6 root    admin   192B Jul 31  2023 4.2-arm64\ndrwxrwxr-x  7 root    admin   224B May 20  2024 4.3-arm64\ndrwxrwxr-x  6 root    admin   192B May 20  2024 4.4-arm64\nlrwxr-xr-x  1 mthieu  admin    50B Dec 28 22:10 Current -&gt; /Library/Frameworks/R.framework/Versions/4.4-arm64\n\n\nYou should see that the folder for your existing R install is still there!\nYou can also see when you ls -l the content of /Library/Frameworks/R.framework/Versions that the “Current” folder, which is what RStudio calls by default, is merely symlinked to a specific R version (see that arrow pointing to one of the version-specific R folders), as opposed to there being only one “Current” folder that is fully overwritten every time you install R. If you change which R version “Current” points to, you could change which version RStudio runs with!\n\n\nInstall the new version of R as usual\nNow that you’ve made your Mac forget that R was ever there, you can run the new .pkg R installer and install freely.\n\nIf you’re paying attention on the second screen of the .pkg installer, you will notice that it gives you instructions about how to “forget” the old R install if you want to stop the force-uninstall behavior. However, they don’t tell you every single pkgutil-listed package you need to pkgutil --forget. If you don’t forget all of the R-associated packages, the installer will overwrite some of the existing R application files, which renders the previous R version “incomplete” and unusable. That’s why you do need to check for every possible R-related package file using pkgutil --pkgs.\n\n\n\nSwitch the active version\nI prefer the RSwitch menu bar GUI utility for switching my active R version.\nIf you want to be hardcore and do it without installing any extra software, the official Posit instructions also tell you that you can manually symlink the “Current” folder to the R version you want to use. For example, this will set me up to use R 4.3:\n# remember, ln syntax puts source first, then link destination\nln -s /Library/Frameworks/R.framework/Versions/4.3-arm64 /Library/Frameworks/R.framework/Versions/Current\nHowever, the Posit instructions also mention that you can use RSwitch, so no need to be a hero and use command line to switch every time 😜\n\n\nAccessing R packages\nEach R version has a separate store for packages. (This makes total sense.) Once you’ve switched versions, how do you get the packages you need for the R version you’re working on? I strongly recommend using the renv package for managing R package environments. While renv does not handle R version switching for you (hence this blog post), its system for managing package stores already smoothly handles packages for different R versions. You can use renv as usual and you should notice no differences as long as you switch to the correct R version before opening your R project. (Conveniently, renv will throw a warning upon startup if the version of R detected is different than the version recorded in the lockfile. If you forget to switch R versions, you can close RStudio, switch the R version, and reopen it.)"
  },
  {
    "objectID": "teaching/teaching-materials.html",
    "href": "teaching/teaching-materials.html",
    "title": "Example teaching materials",
    "section": "",
    "text": "Syllabus\nThis is a prototype syllabus for Intro to Programming for Behavioral Scientists that I designed as part of the Columbia Center for Teaching and Learning’s Innovative Course Design Seminar. The class has not yet been taught and I have not prepared curriculum for it, but I would like to offer it in the future!\nMy overarching goal in designing this syllabus was to lower barriers to student learning, and publicize that goal to students. I expect students to enter my class with no coding experience, and likely an aversion to math & programming topics. I want students to come away from the class believing that programming tools, when wielded properly, are key to producing strong psychological science, and that anyone can produce strong psychological science because anyone can program. While the syllabus definitely won’t achieve all of these goals on its own, it can help set an inclusive tone. I decided to format the syllabus as a website because I felt that would make it visually enticing and readable, and because the website format facilitates posting of R code lecture notes into the same site, allowing the site to be a one-stop reference shop for course info.\nI tried to use an informal but informative tone while writing up course policies to keep those policies clear to all students, irrespective of English language background. In describing those policies, I tried to mention my rationale behind those policies where possible, so that students would understand that all policies, including those that might seem punitive (like not accepting late assignments without prior claiming of a deadline extension), are designed to support student learning.\nI also paid special attention to crafting the following policies:\n\nGrading: Encouraging students to improve their work based on feedback, and use tokens to manage their own deadline extension needs, no questions asked\nEquipment: Reinforcing that students should not self-select out of the class for not having the “right” computer, or a sufficiently expensive one\nCommunication: Spelling out what students should include in help request emails to help them think through their code issue and help me identify their problem quickly\n\nSyllabus website\n\n\n\n\nLesson plan\nThis is a lesson plan for the lab course associated with Research Methods in Psychology, taught in the Columbia psychology department. Approximately 13-17 students are typically enrolled in each lab section. This particular lesson is taught in week 6 of the 13-week semester.\nWhen I TA’d this class in fall 2019, I expanded all lab outlines from basic bullet-point lists of procedural guidelines into full objective-driven lesson plans. I included this lesson plan to illustrate my lesson plan design style. I strive to make lesson plans comprehensive, like a cross between a textbook and a stage manager’s run sheet. In addition to a distinct learning objective, each lab module in the lesson has a timestamp with a suggested run time (and sub-timestamps for longer modules), as well as point-by-point instructions on how to run the module. My junior PhD student colleagues who TA’d the class after me reported using my lesson plans like a textbook, allowing them to quickly parse lab objectives, understand why activities were structured and timed the way they were, and glean procedural tips on how to run the activities. The lessons served not only to structure my own teaching, but also to train the TAs who came after me and to standardize instruction across their sections.\n\n\n\n\n\nMini-lecture video\nThis is a mini-lecture video for Science of Psychology: Explorations and Applications, a semi-flipped class taught in the Columbia psychology department. When we taught the class online in spring 2021, 29 students were enrolled in the class.\nThis video, which illustrates how the brain processes visual information, was assigned along with pre-class readings on visual perception. I included this lesson plan to illustrate how I work with the affordances offered by the teaching environment when creating learning content. I created this animated video to teach a highly visual topic (illustrating neuroanatomy and examples of perceptual information) using visual tools better suited to asynchronous video than synchronous lecture. Students enjoyed watching the video, found the concepts clear (as demonstrated in subsequent reading quizzes and in-class activities), and appreciated being able to pause and replay them at their own pace.\n\n\n\n\nProblem set\nThis is an interactive R problem set for Intro to Programming in R, taught in the Columbia business school. Approximately 40-50 students are typically enrolled in the class. This particular assignment is assigned week 2 and due week 3 of the 6-week half-semester term.\nWhen I TA’d this class in fall 2021, I rewrote weeks 1-3’s assignments to run as interactive browser-based R exercises using the learnr package, instead of their previous form as R Markdown assignments. The key difference between the new and old assignment format is that in their old R Markdown form, students needed to correctly specify a number of technically unrelated code commands (loading packages, document metadata parameters, etc) and correctly implement programming syntax (spelling function and variable names correctly, initializing variables before they are called later, etc.) in order for their documents to render for submission. If there was a syntax error in one answer, the entire document would not render, even if the other answers were correct. In previous semesters of the class, I found that students became frustrated and demoralized when they could not render their homework documents because of such syntax errors, especially when it turned out that one error blocked the entire document from rendering. I realized that familiarity with programming syntax principles was unreasonable to expect students to have mastered that early in the class (and honestly, likely to take many months, if not years, to truly master!). I wanted to rework the first three assignments to run using an engine that would ease their dependence on syntax.\nThere are two main benefits to the new assignment engine. First, each question runs individually, so a syntax error in one question does not block finishing the rest of the assignment. Second, package loading and other programming setup are handled under the hood, so students no longer have to specify those in order to submit their assignments. Later assignments in the term are still administered as R Markdown documents, so students are assessed on those skills after they have had more experience. However, by removing earlier assignments’ dependence on those skills, I brought the assignments more in line with the expected temporal progression toward course learning objectives.",
    "crumbs": [
      "Example teaching materials"
    ]
  },
  {
    "objectID": "teaching/teaching-roles.html",
    "href": "teaching/teaching-roles.html",
    "title": "Teaching roles and responsibilities",
    "section": "",
    "text": "Institution: Spelman College\nTerm: spring 2024\nSupervisor contact: Jimmeka Guillory Wright\nView syllabus\n\n\n\n\nUpper-level undergraduate class, satisfies Advanced Measurement requirement for psychology majors\nStatistics with a regression framework\nAnalysis techniques taught in R/RStudio Cloud\n25 students\n\n\n\n\n\nWrote curriculum\n\nDefined learning objectives\nEvaluated and selected readings\nWrote lesson plans, pre-class reading quizzes, and problem sets\nCreated teaching materials: slides, activity materials, R notebooks\n\nLectured, live-coded, and supervised activities\nGraded assignments\n\nBoth multiple-choice/short answer assignments and R-based computational assignments\nGave detailed formative feedback on problem sets using single-point-style rubrics\n\nHeld weekly office hours\n\n\n\n\n\n\nInstitution: Columbia University\nTerm: summer 2022\nView syllabus\n\n\n\n\nIntro-level undergraduate class, satisfies core science requirement for non-majors\n12 students, with learning teams of 4 students each\n\n\n\n\n\nUpdated curriculum\n\nDefined learning objectives\nEvaluated and selected readings\nEdited lesson plans, pre-class quizzes, and assignment prompts\nCreated and edited teaching materials: slides, activity materials\n\nLectured and led discussions\nGraded writing assignments\n\nGave detailed formative feedback on individual and group writing assignments and projects using single-point-style rubrics\n\nHeld weekly office hours",
    "crumbs": [
      "Teaching roles and responsibilities"
    ]
  },
  {
    "objectID": "teaching/teaching-roles.html#psy-417-statistics-ii-in-psychology",
    "href": "teaching/teaching-roles.html#psy-417-statistics-ii-in-psychology",
    "title": "Teaching roles and responsibilities",
    "section": "",
    "text": "Institution: Spelman College\nTerm: spring 2024\nSupervisor contact: Jimmeka Guillory Wright\nView syllabus\n\n\n\n\nUpper-level undergraduate class, satisfies Advanced Measurement requirement for psychology majors\nStatistics with a regression framework\nAnalysis techniques taught in R/RStudio Cloud\n25 students\n\n\n\n\n\nWrote curriculum\n\nDefined learning objectives\nEvaluated and selected readings\nWrote lesson plans, pre-class reading quizzes, and problem sets\nCreated teaching materials: slides, activity materials, R notebooks\n\nLectured, live-coded, and supervised activities\nGraded assignments\n\nBoth multiple-choice/short answer assignments and R-based computational assignments\nGave detailed formative feedback on problem sets using single-point-style rubrics\n\nHeld weekly office hours",
    "crumbs": [
      "Teaching roles and responsibilities"
    ]
  },
  {
    "objectID": "teaching/teaching-roles.html#psyc-1021-the-science-of-psychology-explorations-and-applications",
    "href": "teaching/teaching-roles.html#psyc-1021-the-science-of-psychology-explorations-and-applications",
    "title": "Teaching roles and responsibilities",
    "section": "",
    "text": "Institution: Columbia University\nTerm: summer 2022\nView syllabus\n\n\n\n\nIntro-level undergraduate class, satisfies core science requirement for non-majors\n12 students, with learning teams of 4 students each\n\n\n\n\n\nUpdated curriculum\n\nDefined learning objectives\nEvaluated and selected readings\nEdited lesson plans, pre-class quizzes, and assignment prompts\nCreated and edited teaching materials: slides, activity materials\n\nLectured and led discussions\nGraded writing assignments\n\nGave detailed formative feedback on individual and group writing assignments and projects using single-point-style rubrics\n\nHeld weekly office hours",
    "crumbs": [
      "Teaching roles and responsibilities"
    ]
  },
  {
    "objectID": "teaching/teaching-roles.html#wistem-data-science-lab-module",
    "href": "teaching/teaching-roles.html#wistem-data-science-lab-module",
    "title": "Teaching roles and responsibilities",
    "section": "WiSTEM Data Science Lab Module",
    "text": "WiSTEM Data Science Lab Module\n\nInstitution: Spelman College\nTerms: summer 2023, summer 2024\nProgram directors: Monica Stephens, Tiffany Oliver\n\n\nCourse details\n\nOffered as part of an application-only 6-week summer bridge accelerator for incoming STEM first-year students\nFull program includes enrollment in summer math and computer science courses for credit, and science lab courses not for credit (including this course)\nStudents learned basic data manipulation and visualization skills in R and completed independent lab projects\n\n\n\nInstructional responsibilities\n\nCreated curriculum\n\nDefined learning objectives\nCreated all lesson plans and teaching materials\n\nLectured, supervised lab work, and advised on independent projects",
    "crumbs": [
      "Teaching roles and responsibilities"
    ]
  },
  {
    "objectID": "teaching/teaching-roles.html#psyc-1021-the-science-of-psychology-explorations-and-applications-1",
    "href": "teaching/teaching-roles.html#psyc-1021-the-science-of-psychology-explorations-and-applications-1",
    "title": "Teaching roles and responsibilities",
    "section": "PSYC 1021 – The Science of Psychology: Explorations and Applications",
    "text": "PSYC 1021 – The Science of Psychology: Explorations and Applications\n\nInstitution: Columbia University\nTerm: spring 2021\nLead instructor: Caroline Marvin\n\n\nCourse details\n\nAs above\nApproximately 30 students, with learning teams of approximately 4 students each\n\n\n\nInstructional responsibilities\n\nDeveloped curriculum, in conjunction with lead instructor\n\nDefined learning objectives\nEvaluated and selected readings\nWrote lesson plans, pre-class quizzes, and assignment prompts\nCreated teaching materials: slides, activity materials, mini-lecture videos\n\nGuest lecturing (approximately 1/4 of instructional modules)\nGraded writing assignments\n\nGave detailed formative feedback on individual and group writing assignments and projects using single-point-style rubrics\n\nHeld weekly office hours",
    "crumbs": [
      "Teaching roles and responsibilities"
    ]
  },
  {
    "objectID": "teaching/teaching-roles.html#psychology-department-coding-bootcamp",
    "href": "teaching/teaching-roles.html#psychology-department-coding-bootcamp",
    "title": "Teaching roles and responsibilities",
    "section": "Psychology department coding bootcamp",
    "text": "Psychology department coding bootcamp\n\nInstitution: Columbia University\nTerms: spring 2018, fall 2018, fall 2019, fall 2020, fall 2021\n\n\nCourse details\n\nUnofficial open-enrollment workshop for psychology students at the undergrad level and above\n5-20 students per session, depending on the semester\n\n\n\nInstructional responsibilities\n\nCurriculum development\n\nDrafted lessons for 2 half-day sessions\nPeer-reviewed 1 session lesson drafted by another co-instructor\n\nLecture: Live-code lectured some or all of the lessons I prepared (depending on the semester)",
    "crumbs": [
      "Teaching roles and responsibilities"
    ]
  },
  {
    "objectID": "teaching/teaching-roles.html#psyc-1490-research-methods-in-psychology",
    "href": "teaching/teaching-roles.html#psyc-1490-research-methods-in-psychology",
    "title": "Teaching roles and responsibilities",
    "section": "PSYC 1490 – Research Methods in Psychology",
    "text": "PSYC 1490 – Research Methods in Psychology\n\nInstitution: Columbia University\nTerms: fall 2018, fall 2019\nLead instructor: Katherine Fox-Glassman\n\n\nCourse details\n\nUpper-level undergraduate class, required for the psychology major\nApproximately 70 students in the entire class, with lab sections of approximately 15 students\n\n\n\nInstructional responsibilities\n\nUpdated course materials\n\nWrote 2 new R lesson plans and 2 new associated problem sets\nUpdated and annotated existing lesson plan outlines for each lab class\nCreated slide decks for each lab class\n\nTaught lab section\nGraded assignments\n\nExam short-answer questions (for all students)\nMethods problem sets in R (for students in my section)\nScience writing assignments and projects (for students in my section)\nIndividual and group presentations (for students in my section)\n\nHeld weekly office hours\nWrote short-answer exam questions",
    "crumbs": [
      "Teaching roles and responsibilities"
    ]
  },
  {
    "objectID": "teaching/teaching-roles.html#foundations-for-research-computing-coding-bootcamp",
    "href": "teaching/teaching-roles.html#foundations-for-research-computing-coding-bootcamp",
    "title": "Teaching roles and responsibilities",
    "section": "Foundations for Research Computing coding bootcamp",
    "text": "Foundations for Research Computing coding bootcamp\n\nInstitution: Columbia University\nTerms: fall 2019, spring 2020, winter 2022\nCoordinator: Patrick Smyth\n\n\nCourse details\n\nOpen-enrollment workshop hosted by Columbia Libraries for graduate students and postdocs across the university\n30 students per session (in-person), 50 students per session (online)\nAssociated with The Carpentries\n\n\n\nInstructional responsibilities\n\nDelivered live-code lecture sessions introducing students to basic file navigation commands in Unix shell (1 3-hour session per workshop)",
    "crumbs": [
      "Teaching roles and responsibilities"
    ]
  },
  {
    "objectID": "teaching/teaching-roles.html#busi-b8144-intro-to-programming-in-r",
    "href": "teaching/teaching-roles.html#busi-b8144-intro-to-programming-in-r",
    "title": "Teaching roles and responsibilities",
    "section": "BUSI B8144 – Intro to Programming in R",
    "text": "BUSI B8144 – Intro to Programming in R\n\nInstitution: Columbia University\nTerms: spring 2019, spring 2020, fall 2021\nLead instructor: Jared Lander\n\n\nCourse details\n\nMaster’s level half-semester elective course in the business school\nLecture class with 40-70 students depending on the term\n\n\n\nInstructional responsibilities\n\nUpdated course materials\n\nRevised existing problem sets to align objectives with lecture content\nRe-implemented problem sets using learnr for students to complete exercises online\n\nGraded and gave written feedback on R problem sets\nHeld weekly office hours\nSupported students’ work over email",
    "crumbs": [
      "Teaching roles and responsibilities"
    ]
  },
  {
    "objectID": "teaching/teaching-roles.html#psyc-1001-science-of-psychology",
    "href": "teaching/teaching-roles.html#psyc-1001-science-of-psychology",
    "title": "Teaching roles and responsibilities",
    "section": "PSYC 1001: Science of Psychology",
    "text": "PSYC 1001: Science of Psychology\n\nInstitution: Columbia University\nTerm: fall 2017\nLead instructor: Kathleen Taylor\n\n\nCourse details\n\nIntro-level undergraduate class, satisfies core science requirement for non-majors\nApproximately 100 students\n\n\n\nInstructional responsibilities\n\nGraded exam essays\nWrote multiple-choice and essay exam questions\nHeld weekly office hours\nDelivered guest lecture on memory",
    "crumbs": [
      "Teaching roles and responsibilities"
    ]
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "In accordance with my IRB protocols, I do not advertise my own research studies on my website. However, if you’re interested in participating in research in general there are many other studies you can take part in.\nYou can participate in online research studies from researchers all over the world at prolific.co. They’re based in the UK, but many studies are posted for participants in the US as well.\nYou can participate in local in-person research studies at a university near you. Check out your local university’s psychology department and medical school websites to find out what kinds of research studies they’re recruiting for."
  },
  {
    "objectID": "research.html#recruiting-online-participants",
    "href": "research.html#recruiting-online-participants",
    "title": "Research",
    "section": "Recruiting online participants",
    "text": "Recruiting online participants\nI run online studies on prolific.co, a researcher-friendly website for online behavioral study recruitment that’s kind both to researchers and to participants. All my contacts with participants have been pleasant, and the Prolific support staff are so responsive and helpful. You can get a credit towards paying your first batch of participants if you sign up with my link here!"
  },
  {
    "objectID": "research.html#running-behavioral-tasks-online",
    "href": "research.html#running-behavioral-tasks-online",
    "title": "Research",
    "section": "Running behavioral tasks online",
    "text": "Running behavioral tasks online\nI use gorilla.sc, a site that lets you build behavioral tasks both through GUI and code (their software is based on JavaScript/JQuery, though you don’t need to know either of those languages to use their tools). Their site is easy to use for JavaScript novices (like me), and still allows for full customization through added code. Their support staff are stellar as well! Instead of paying a subscription fee, you pay about a dollar per each participant who completes a task built on their platform. If you’d like to sign up, you can run twenty participants free with a referral. (email me for a referral link; they don’t currently have a referral link set up. PS: An academic email address is required to claim the bonus.)"
  },
  {
    "objectID": "research.html#postdoctoral-research",
    "href": "research.html#postdoctoral-research",
    "title": "Research",
    "section": "Postdoctoral research",
    "text": "Postdoctoral research\n\nHuman affective responses to looming motion\nRapidly approaching objects, like incoming predators or projectiles, are often dangerous. Many species of animals can detect and avoid the specific patterns of looming visual motion associated with such objects, using quite similar neural mechanisms.\nExperience some looming motion with this video:\n\n\nFor example, mammals encode looming motion in the superior colliculus, a region of the midbrain that coordinates rapid reorienting to salient signals in the environment. In this project, I am using computational models from nonhuman animal research (for example, fruit flies!) to investigate how humans respond to looming motion:\nHow does looming motion in videos predict people’s emotional responses to those videos? Preliminary results suggest that the “loominess” of short, emotionally evocative videos can predict how people label their emotional responses to those videos (choices like “fear”, “joy”, “anxiety”, “excitement”, etc.). Specifically, patterns of looming-predicted emotional responses look like responses made based on subjective arousal, or how “activated” people feel when watching the videos.\nHow does the human superior colliculus encode looming motion? Preliminary results suggest that BOLD fMRI activity in the human superior colliculus does track with the “loominess” of videos–in particular, even boring videos of expanding rings typically used to map out visual space in the brain. We are currently conducting an fMRI study to look further at how superficial and deep layers of the superior colliculus might track different visual features of those videos, like retinotopy (where in the visual field is stuff happening?) vs. specific looming motion (is something expanding like it’s coming toward me, anywhere in the visual field?).\nHow does the human superior colliculus encode people’s emotional responses to naturalistic looming motion? We are currently conducting an fMRI study to test this question! We are showing people emotionally evocative videos of naturalistic looming (animals jumping at the camera… like the video above!), and then asking them to report how each video makes them feel. This way, we can directly compare patterns of BOLD activity in the superior colliculus based on what animals people are watching, whether those animals are looming, and how people feel as they watch."
  },
  {
    "objectID": "research.html#graduate-school",
    "href": "research.html#graduate-school",
    "title": "Research",
    "section": "Graduate school",
    "text": "Graduate school\nCheck out my dissertation for the (very) nitty-gritty of these projects!\n\nIndividual differences in age perception\nWhen we see another person’s face, we pretty quickly estimate their race, gender, and age just from looking at their face. When it comes to age in particular–we can estimate someone’s age in years, but we can also label them as “young” or “old”. How do people categorize faces of different ages, and do different people categorize age differently? We show participants a series of faces made to appear young, old, and somewhere in the middle, and ask them to label each face as “young” or “old”.\n\n\n\nSome example age- and gender-morphed faces. (People don’t actually see them animated in the study, just one at a time!)\n\n\nCheck out the “poster” video below for preliminary results on how the brain patterns of younger participants (in their 20s) change when judging faces based on their age vs. gender, or using binary vs. slider judgments.\n\n\n\n\nMeasurement of emotion self-reporting\nWhat emotion are you feeling right now? When you’re experiencing some emotion, you could probably report what it is you’re feeling without much difficulty–this is something that humans can do readily. But how do we turn our subjective emotional experience into self-report, and how does this relate to other parts of our emotional lives? We’ve developed a psychophysics-style paradigm where we show participants a series of brief, emotionally evocative GIFs, ask them to choose one of two emotion words to describe each experience (e.g. “neutral” or “fear”), and then fit their responses to a psychometric function. This way, we can find out how fearful (for example) a GIF has to be for an individual to say they feel fear (their psychometric threshold). We’ve also administered a series of surveys on things like mood, depression/anxiety tendencies, and more, so we can begin to examine how emotion-labeling patterns (as indexed by the psychometric task) predict facets of people’s social & emotional lives.\nWe’ve run this first as a behavioral study, where results indicate that people who report being more emotionally expressive in their day-to-day lives have lower thresholds for reporting emotional responses to our GIFs (see this poster).\nWe also ran an fMRI study to investigate whether people’s neural representations of subjective emotional experiences, specifically ambiguous, mixed-emotion experiences, change based on how they report their emotional response to said experiences. Preliminary results indicate that fMRI patterns for mixed emotional experiences appear not as a blend of positive and negative patterns, but as a distinct pattern that may arise from the process of resolving emotional ambiguity. (see this poster)."
  },
  {
    "objectID": "research.html#before-graduate-school",
    "href": "research.html#before-graduate-school",
    "title": "Research",
    "section": "Before graduate school",
    "text": "Before graduate school\n\nAttention, memory, and media multitasking\nMedia multitasking, or the act of engaging with multiple streams of media simultaneously, something. (If you’re listening to music while reading this, you’re engaging in media multitasking right now!) In the Stanford Memory Lab, we’ve demonstrated that individuals who chronically media multitask show a reduced working memory capacity, and that those people also show reduced long-term memory for the items they seem to be having more difficulty maintaining in working memory. What are the neural mechanisms subserving this relationship, and how do individual differences on a neural level track with trait media multitasking? It’s possible that the degree to which people favor the use of top-down, goal-directed attention vs. bottom-up, salience-reorienting attention might predict someone’s tendency to perform worse on these working memory and long-term memory tasks. At the Stanford Memory Lab, I helped use fMRI to investigate how activation in top-down dorsal attention networks and bottom-up ventral attention networks might predict whether someone will successfully engage with and remember a given stimulus. They are conducting a follow-up EEG project to investigate whether individual differences in power in certain EEG frequency bands, which might track top-down pre-stimulus preparatory attention, predict the degree to which someone will successfully engage with and remember a given stimulus.\n\n\nHealthy aging and associative memory\nEveryone experiences changes in cognition as they get older. We understand that some people develop neurodegenerative disorders as they age, such as Alzheimer’s disease and dementia, and that these pathological changes are associated with marked deficits in cognition. But this isn’t inevitable for people as they age. What kinds of cognitive changes should we expect as we age? What changes are nothing to worry about, and what changes are of medical concern? Existing research indicates that associative memory, the particular task of binding previously unrelated features into a single memory representation, may be a “canary in the coal mine” for pre-clinical cognitive decline. (When you run into an acquaintance at the grocery store, and you can remember the context in which you know them, but not what their name is, that’s your associative memory that’s failing you.) At the Stanford Memory Lab, I assisted with a large-scale individual differences study to try to describe how changes in cognition (associative memory in particular) relate to clinical markers of neurodegeneration in cognitively healthy older adults.\nYou can read the published results (to date) of this research here!"
  },
  {
    "objectID": "posts/2024-05-20-conda-env-export-from-history.html",
    "href": "posts/2024-05-20-conda-env-export-from-history.html",
    "title": "Export your conda envs –from-history !",
    "section": "",
    "text": "Hi everyone! It’s been 3 years since I’ve put up a blog post…\n\n\n\nvia GIPHY\n\nBut I’m trying to get back into the practice of documenting tips/tricks/hacks I’ve assembled in the process of managing my research computing, so that you don’t have to struggle like I did!\nRight now, I’m currently cleaning up the analysis code repository associated with my first published postdoctoral research project. Accordingly, I’m hoping to put up several little #hacks blog posts up to document various roadblocks/bypasses I’m encountering as I attempt to make the analysis code end-to-end reproducible for another user.\nI’ve used so many other scientists’/programmers’ helpful blog posts in setting up the pipeline, so I hope I can pay it forward!"
  },
  {
    "objectID": "posts/2024-05-20-conda-env-export-from-history.html#im-alive",
    "href": "posts/2024-05-20-conda-env-export-from-history.html#im-alive",
    "title": "Export your conda envs –from-history !",
    "section": "",
    "text": "Hi everyone! It’s been 3 years since I’ve put up a blog post…\n\n\n\nvia GIPHY\n\nBut I’m trying to get back into the practice of documenting tips/tricks/hacks I’ve assembled in the process of managing my research computing, so that you don’t have to struggle like I did!\nRight now, I’m currently cleaning up the analysis code repository associated with my first published postdoctoral research project. Accordingly, I’m hoping to put up several little #hacks blog posts up to document various roadblocks/bypasses I’m encountering as I attempt to make the analysis code end-to-end reproducible for another user.\nI’ve used so many other scientists’/programmers’ helpful blog posts in setting up the pipeline, so I hope I can pay it forward!"
  },
  {
    "objectID": "posts/2024-05-20-conda-env-export-from-history.html#todays-tip-export-your-conda-environments-from-history",
    "href": "posts/2024-05-20-conda-env-export-from-history.html#todays-tip-export-your-conda-environments-from-history",
    "title": "Export your conda envs –from-history !",
    "section": "Today’s tip: Export your conda environments –from-history",
    "text": "Today’s tip: Export your conda environments –from-history\n\nThe scenario\nIf you use conda to manage Python package environments associated with specific analysis project folders (and you should!), you’ll know that you can export a environment.yml config file that records all of the packages installed in your environment using the following terminal command:\nconda env export &gt; environment.yml\nwhere conda env export will generate the config information associated with your active environment, and &gt; environment.yml captures the text output and saves it in said file.\nThen, another user (or you, but in a different folder) can recreate your package environment by saving a copy of environment.yml into that new project folder and running the following in a terminal:\nconda env create -f environment.yml\n\n\nThe problem\nIn a perfect world, this goes off with no hitches!\nHOWEVER, if the exported environment is from a machine running a different OS than the OS on which you are creating the new environment, you are likely to run into problems if you follow these instructions as written. (I encountered this use case because I do my main work on our lab’s Linux computing cluster, but I prefer to generate ggplot figures on my local MacBook, and so I keep GitHub-tracked copies of the repository on the lab server and on my laptop.)\nThis is because the default behavior of conda env export is to export all installed packages to the config record, including OS-specific dependencies. If you attempt to conda env create from, say, a Mac-generated environment.yml file on a Linux machine (as I did), you will get errors that some Mac-specific packages aren’t available to install, and the environment creation will fail.\nYou might be thinking, “But wait! None of the Python packages I explicitly installed in my environment are OS-specific. numpy/pandas/matplotlib/pytorch/what have you are all supposed to work on any operating system!”\nThat is true, because the OS-specific dependencies are being hidden from you! If you just run, say, conda install pandas, you can install the same pandas version on Windows, Mac, or Linux, but pandas’ underlying dependency packages (that will be installed along with the package you asked for) might differ from OS to OS.\n\n\nThe solution\nYou can add the --from-history flag to your conda env export call to export a lighter-weight version of the config info to environment.yml.\nconda env export --from-history &gt; environment.yml\nInstead of writing out your package environment with all of the nitty-gritty OS-specific package info, the --from-history flag tells conda env export to record only the packages that were explicitly installed using conda install. Thus, environment.yml will record the endpoint packages you told conda to install, but not the (OS-specific) package dependencies that come along for the ride.\nThis is covered briefly in the conda docs, but it’s really easy to miss if you’re not specifically looking for it–hence this blog post.\nNow, when you port that environment.yml onto another machine running a different operating system, you should be able to recreate the environment without running into cross-OS errors!\n(Obviously, you might still get a different error. /shrug but it won’t be this one, hopefully.)\nHappy environment creating!\n\n\nPS: Refine environment.yml by hand\nAnother behavior associated with the --from-history flag is that it only records the package versions (or lack thereof) that you explicitly specified upon install. For example, if you specifically called, like:\nconda install pytorch=1.12.1\nto install that specific version of PyTorch, then your environment.yml file will record pytorch=1.12.1 accordingly, but if you only called:\nconda install pytorch\nthen your environment.yml file will only record pytorch without a specific version associated.\nOne of the purposes of recording package environments is to specify package versions, just in case a package update introduces a feature change that causes code not to reproduce as written. By default, conda env export does include package version information, which we want, but it includes it alongside OS-specific information which we don’t want.\nIf you want to add back in package version information, but it wasn’t originally caught/logged when you ran conda env export --from-history, you can manually edit the environment.yml and add in package version info or specific conda channel info that will be used when someone else conda env creates an environment from your file.\n(Please remember that if you later run conda env export --from-history &gt; environment.yml again, conda will overwrite any of your hand edits to the previous environment.yml unless you specify a different path for the new environment.yml to be overwritten! And then, yes, you will need to manually re-add in your package version/channel specs. I really hate this, but it only takes a few seconds as long as you remember to do it.)"
  },
  {
    "objectID": "posts/2021-05-28-intro-psych-brain-bingo/index.html",
    "href": "posts/2021-05-28-intro-psych-brain-bingo/index.html",
    "title": "Teaching introductory psychology: Brain bingo",
    "section": "",
    "text": "This post will show you my motivation and methods for designing and running a game of Zoom bingo, but for basic neuroanatomy! This activity would be appropriate for an introductory psychology or neuroscience class. The instructions below are specific to an online class (the activity was run in the spring 2021 semester), but can be simplified to run in-person. (You can skip all the streaming stuff; that’s the most complicated part anyway.)"
  },
  {
    "objectID": "posts/2021-05-28-intro-psych-brain-bingo/index.html#choose-key-terms",
    "href": "posts/2021-05-28-intro-psych-brain-bingo/index.html#choose-key-terms",
    "title": "Teaching introductory psychology: Brain bingo",
    "section": "Choose key terms",
    "text": "Choose key terms\nIn some way, this is the most important part of the activity. By choosing the key terms to quiz in bingo, you are deciding which key terms you want students to know. Don’t take this lightly! (Backward design is everywhere.) I kept this in mind when deciding which terms to omit. Intro-level students don’t need to be neuroanatomy experts; they only need to know enough to be able to orient themselves when we discuss cognitive neuroscience research in class.\nFour our class, we ended up selecting a combination of the key terms presented in the Brain chapter of our course’s textbook, and a few more based on the judgment of the teaching team (like MRI slice orientations)."
  },
  {
    "objectID": "posts/2021-05-28-intro-psych-brain-bingo/index.html#prepare-bingo-cards",
    "href": "posts/2021-05-28-intro-psych-brain-bingo/index.html#prepare-bingo-cards",
    "title": "Teaching introductory psychology: Brain bingo",
    "section": "Prepare bingo cards",
    "text": "Prepare bingo cards\nColumbia has a subscription to the G Suite for Education, which means we have access to Jamboard, Google’s interactive whiteboard app. A whiteboard app like Jamboard isn’t strictly necessary for an activity like this, but it is useful. We could send students a link to the Jamboard file and direct them to their respective bingo cards on the large whiteboard document, which we could also have open to monitor their progress as they used built-in whiteboard features to mark bingo spots as “called”.\nUltimately, Jamboard or not, students just need unique bingo cards that they can annotate somehow to mark when key terms have been called. Bingo cards can even be static screenshot images, with which students would use a markup tool on their own device or mark corresponding locations on a 4x4 grid on a sheet of paper.\nI used Jamboard’s sticky note feature to create a 4x4 grid of sticky notes on the first frame (or “slide”) of a file. I typed in 16 key terms, one in each sticky note, and clicked/dragged to physically shuffle the sticky notes around until the board looked sufficiently pseudo-random.\n\nTo create additional unique bingo cards, I duplicated an existing bingo card, replaced some key terms with other key terms (to ensure that not every bingo card had the exact same items, and so students can’t assume every key term appears on their card), and manually shuffled the sticky notes again.\nFinally, just in case students encountered Jamboard access issues, I screenshotted every unique bingo card and saved all the labeled screenshots in a folder for students to access just in case."
  },
  {
    "objectID": "posts/2021-05-28-intro-psych-brain-bingo/index.html#prepare-bingo-calls",
    "href": "posts/2021-05-28-intro-psych-brain-bingo/index.html#prepare-bingo-calls",
    "title": "Teaching introductory psychology: Brain bingo",
    "section": "Prepare bingo calls",
    "text": "Prepare bingo calls\nI wanted the ability to test students’ recall using both visual cues (e.g. an arrow pointing in a particular anatomical direction) and verbal cues (e.g. a description of a brain region’s key functions). The simplest way I found to do this was to create a Google Slides presentation (PDF export here, contact me for the Google Slides themselves), where each slide contained a recall cue for one key term. For anatomical directions, imaging slice orientations, and lobes of the cerebrum, I made picture cues. For other brain structures/regions, including subcortical structures, I wrote a short one-sentence description of that region’s definition/function based on what students would have seen in the textbook. Each key term had only one cue slide associated with it.\nI made these slides in sensible order just to make it easier for me to catalogue all the terms I was adding. To randomize them for bingo presentation, I downloaded the Slides Randomizer Google Slides plugin and followed its instructions to randomize the order of all slides (excluding the first slide, on which I typed some instructions). After randomizing, I manually re-ordered a few slides so that there wouldn’t be large chunks of consecutive picture cues or verbal cues."
  },
  {
    "objectID": "posts/2021-05-28-intro-psych-brain-bingo/index.html#prepare-computer-for-streaming",
    "href": "posts/2021-05-28-intro-psych-brain-bingo/index.html#prepare-computer-for-streaming",
    "title": "Teaching introductory psychology: Brain bingo",
    "section": "Prepare computer for streaming",
    "text": "Prepare computer for streaming\nWe (the teaching team) had previously decided to run the activity in bingo teams of approximately 4 students. We did this both to allow students to assist each other within teams, and to acclimate students to group work in the class before they were assigned to semester-long project groups the next week of class.\nOrdinarily, running an activity like this would only require the instructor screen-sharing into Zoom and students filling out their bingo cards in a separate browser window. However, because we wanted students to work in teams, we had to figure out an alternative that allowed students to see a screen share while in breakout rooms. We settled on me streaming my screen to a web link that students could have open while they were in breakout rooms.\nFor the entire semester, I Zoomed into class from my (2017) iMac desktop in my office. (Per our lab’s COVID protocols, only one person was allowed in each office at a time. I would book the office for class time twice a week and leave other times of the week for others to work there.) The iMac provided two main benefits over joining class from my laptop in my apartment: gigabit wired internet on campus and a really big screen. Both of these features proved key to streaming brain bingo successfully. The internet speed helped ensure the stream wouldn’t be patchy, and the large screen allowed me to tile more windows on screen so I could maintain a “control board” for all the programs I needed to keep an eye on.\n\nStreaming destination: configure YouTube Live\nBetween all the different streaming sites I could use, I figured YouTube Live would be the easiest. I already had a YouTube account associated with my Columbia Gmail, and it would be easy enough to send the link to students to view without needing to log in themselves.\n\n\nStreaming source: configure StreamLabs app\nHaving never streamed anything before, I didn’t realize you can’t just share your screen and webcam directly to a stream. You can stream just your webcam, but not with a screen share. Zoom makes it look so easy! I needed to download an encoder, or a tool that would construct a feed from multiple sources that could be sent into a stream. (When Zoom talks with webcam/sceen share are livestreamed to something like YouTube Live, Zoom itself acts as the encoder in this instance.) Based on the relevant YouTube support page, I decided to try the Streamlabs app.\nFollowing Streamlabs’s basic setup guide for Mac, I added my YouTube login info to authenticate my future stream, and then did the absolute bare minimum to setup a stream that would show two sources: me on webcam/audio (the default), and a Window Capture of my Google Slides presentation with the brain cue slides.\n\n\nConnecting the source to the destination\nThe last thing I needed to do to hook Streamlabs up to my YouTube Live account was plug in the stream key and stream URL. Following a combination of Streamlabs’s guide and YouTube’s instructions to stream from an encoder to a scheduled stream, I copied both pieces of information from the YouTube Live admin editing page for the stream and pasted them into Streamlabs. Clicking “Go Live” in Streamlabs made all my stuff start showing up in YouTube! Wow!!!"
  },
  {
    "objectID": "posts/2021-05-28-intro-psych-brain-bingo/index.html#run-the-activity",
    "href": "posts/2021-05-28-intro-psych-brain-bingo/index.html#run-the-activity",
    "title": "Teaching introductory psychology: Brain bingo",
    "section": "Run the activity",
    "text": "Run the activity\n\nPrep students\nWhile students were in the main Zoom (and we could reach everyone using the chat), we introduced the review game and sent the links they would need in their breakout rooms:\n\nYouTube Live link to the stream\nJamboard file link for all the interactive bingo cards\nGoogle Drive folder link for static bingo card screenshots (in case of technical difficulty with Jamboard)\n\nThen, when everyone was ready, we used Zoom’s random breakout room assignment feature to send students to team breakout rooms. (Our instructor and the other TA stayed in the main Zoom in case of troubleshooting.)\n\n\nStart the stream!\nStarting the stream would prevent me from using my desktop webcam for Zoom and streaming, so I had to either join the class Zoom on another device to keep an eye on it, or just leave and hope people could communicate with me through the stream chat. I happened to have my laptop with me in my office, so I managed to join the class Zoom on my laptop while closing out of Zoom on my work desktop to start up the stream. I hit “Go Live” on Streamlabs (since I’d already configured the stream link and Streamlabs connection on YouTube Live before class) and we were rolling!\nOnce on the stream, I introduced the instructions, answered any questions that came in through the stream chat or from the rest of the teaching team in the main Zoom, and then started bingo! I allowed 30 seconds per bingo call–hopefully long enough to give students sufficient time to identify the associated key term, but fast enough to keep the activity moving.\nFor each bingo call, I announced the photo or the verbal description (e.g. “the next term is THIS anatomical direction”), and then just… kept talking through the 30 seconds for each cue. I tried to announce when 15 seconds and 5 seconds remained for each cue, and made comments and jokes in the rest of the time. For example, if the relevant key term was a lobe of the cerebrum, I might ad-lib about functions associated with that lobe to give students extra “flavor” information.\nI know this sounds so glib, but my biggest tip for success in the actual stream is to look like you’re having fun! Here, adopting advice from Twitch actually helps. Not necessarily to “plug your channel” (I mean, students are enrolled in your class already, no need to plug), but to get in the rhythm of the strange 1.5-sided conversation that occurs between a streamer and viewers. (I think it’s actually quite related to the 1.5-sided conversation between an instructor and students, but that’s another blog post…)\nMy second biggest tip is to make sure you are keeping an eye on all necessary windows: the slides, the timer for each slide, the stream chat, and any message channel with the teaching team (incase they notice a technical difficulty).\nOverall, I had a lot of fun running it, and students reported that they enjoyed it too! The weirdest thing for me to get used to was the 5-ish second delay between me saying something and it getting streamed to students. When they commented in the chat (which was an effective way to hear from them because we couldn’t see their Zoom messages from their breakout rooms!), I had to mentally adjust for comment lag in connecting what they’d typed to what I’d said. Not a huge deal, but I had to remind myself to wait for several seconds after I said something before expecting comments.\nIf you want, you can watch a replay of my stream and see how engaging (lol) you think I managed to be!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Monica Thieu on the internet",
    "section": "",
    "text": "I am a postdoctoral fellow in cognitive neuroscience at Emory University, studying how perceptual and conceptual information contribute to emotional experiences.\nI use primarily R for my own data analysis and teach R so others might use it as well.\nWelcome to my quarto-powered website! Please take a look around."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "Where I’m going!\nIn fall 2025, I will be starting as an Assistant Professor in the department of Neuroscience, Cognition, and Behavior at the University of San Diego in sunny San Diego, CA.\nIf you are a USD undergraduate, I will be recruiting research assistants. And for external scientists, I am always open to potential research collaborations. Please contact me!\nMy lab website is under construction! Please check back for updates.\n\n\nWhere I am.\nI’m currently a FIRST-funded postdoctoral fellow working in Dr. Phil Kragel’s ECCO Lab at Emory University in Atlanta, GA. In my research, I apply deep learning models to behavior and neuroimaging to investigate how people experience emotions from the things they see and hear. In my teaching, I develop courses on the foundations of those computational techniques for junior behavioral scientists. At any given moment, I’m probably drinking from a very large cup of tea.\nSadly, the NIH IRACDA postdoctoral training program, of which FIRST at Emory was a part, has been terminated nationwide along with many other scientific training grants as of April 2025. I am proud to be one of the final fellows whose research and teaching training were funded by FIRST.\nPreviously:\nIn 2013 I started working as a research assistant on a media multitasking & cognition project with Dr. Melina Uncapher, then a research associate in Dr. Anthony Wagner’s Memory Lab, during my freshman year at Stanford University. She couldn’t get rid of me, and I continued working with her in the lab throughout undergrad, earning two undergraduate summer fellowships through Stanford (PsychSummer and Bio-X) so I would literally never have to leave. I completed my bachelor’s degree in three years, serving as an undergraduate Teaching Fellow for Psych One, Stanford’s introductory psychology course, during my third year, while also completing an honors thesis under Drs. Uncapher and Wagner.\nAfter graduation in 2015, I transitioned into a full-time lab manager + research assistant position in the Memory Lab. As lab manager/RA, I served as a jack-of-most-trades, handling various arms of data collection and analysis for our healthy aging study, maintaining lab needs both tangible (office supplies, copies, uncomfortable amounts of coffee) and intangible (IRB protocols, software licenses, moral support), and defusing tense meetings with horrible jokes. I also spent some time assisting Dr. Uncapher with data processing and statistics for her educational neuroscience projects with the Neuroscape group at UC San Francisco.\nIn fall 2017, I began working on my PhD in Dr. Kevin Ochsner’s Social Cognitive & Affective Neuroscience Lab at Columbia University, which I completed in spring 2022. From 2017-2019, I co-managed an informal R users club in the department, where I helped to organize a yearly coding bootcamp and bi-weekly meetings/workshops to enrich our collective R experience. During my first post-PhD summer, I taught The Science of Psychology: Explorations and Applications, a class I originally co-designed with Dr. Caroline Marvin, at Columbia.\n\n\nSome of my code.\nCheck out my GitHub to see the code projects I’ve contributed to.\n\n\nA snapshot of my CV.\n(Click here for the full-page version. Made with the vitae package!)"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact me",
    "section": "",
    "text": "Email\nFor professional (academic or consulting) and media inquiries you can reach me at mthieu -at- emory -dot- edu.\nUnfortunately I cannot respond to fan mail, but I appreciate the thought!\n\n\nTipping (if you found my posts useful)\nI put coding tips and tutorials on my blog for free so that they can reach anyone who might find them useful! These blog posts do take a non-negligible amount of time to write. If my work has helped you in any way, I will happily accept a small tip if you have the financial resources.\n\n\n\n\nTwitter\nI used to post about stats/science on occasion @monica_too_, but I have stepped away from Twitter since it was taken private. :("
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog posts",
    "section": "",
    "text": "Migrating this website from blogdown to Quarto\n\n\n\n\n\n\nR\n\n\nquarto\n\n\nhtml\n\n\nhacks\n\n\n\n\n\n\n\n\n\nDec 29, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHow to install and switch between multiple R versions on a Mac\n\n\n\n\n\n\nhacks\n\n\nr\n\n\n\nIt’s really quite smooth… once you have it set up.\n\n\n\n\n\nMay 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nExport your conda envs –from-history !\n\n\n\n\n\n\nhacks\n\n\npython\n\n\nconda\n\n\n\nA quick helpful hint from personal experience on conda environment management.\n\n\n\n\n\nMay 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPractice ggplot2 aesthetics with learnr\n\n\n\n\n\n\ntutorial\n\n\nR\n\n\nplotting\n\n\ninteractive\n\n\n\nA learnr interactive tutorial to practice adjusting aesthetics of ggplot2 plots.\n\n\n\n\n\nJul 29, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nTeaching introductory psychology: Brain bingo\n\n\n\n\n\n\ntutorial\n\n\nteaching\n\n\n\nInstructions for running a brain bingo activity in class suitable for intro-psych level students.\n\n\n\n\n\nMay 28, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nExploring the NYPD Misconduct Complaint Database\n\n\n\n\n\n\nR\n\n\ngit\n\n\ntidyverse\n\n\nslides\n\n\n\nThis is the landing page for slides for my presentation with my colleague Paul Bloom.\n\n\n\n\n\nNov 12, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nHow to prepare and teach an R lesson\n\n\n\n\n\n\nR\n\n\ntidyverse\n\n\ntutorial\n\n\nteaching\n\n\n\nThis is the outline for my 2020 NYC R Conference talk. Come on in!\n\n\n\n\n\nAug 15, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nWrangling multilevel data in the tidyverse\n\n\n\n\n\n\nR\n\n\ntidyverse\n\n\ntutorial\n\n\n\nThis is the narrative lesson plan and tutorial for my 2-hour R workshop, Wrangling Multilevel Data in the R Tidyverse. Come on in!\n\n\n\n\n\nApr 8, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2021-07-29-cu-sipps-ggplot-aes.html",
    "href": "posts/2021-07-29-cu-sipps-ggplot-aes.html",
    "title": "Practice ggplot2 aesthetics with learnr",
    "section": "",
    "text": "Do you or early-intermediate R users you know want to practice beautifying your ggplot2 plots for attractiveness and intelligibility? Would you like to do this using interactive exercises that don’t require you to download anything or even open RStudio? Have I got the tutorials for you!\nI made these interactive worksheets for the Columbia Dept of Psychology’s Summer Internship Program in Psychological Science, a fantastic trainee-led pilot program supporting structured summer research assistantships in the department. Since summer 2021 is the first summer this program has gone live with participants from assorted labs, several grad students have volunteered to contribute new teaching materials for sessions that haven’t already been taught elsewhere in the department.\nBoth worksheets are interactive learnr tutorials that allow you to run your own code snippets for each plot adjustment exercise, and check your answers using gradethis. Please check them out and pass them along to learners who might be interested!\nSmall note: The tutorials are hosted on my free shinyapps.io account, which has a decent but frugal processing limit. If lots of people happen to have the page open, you may experience timeouts, but it’s pretty unlikely unless these blow up online somehow!\n\nWorksheet 1: Axes, labeling, and color mapping\n\n\nWorksheet 2: Annotation and theme elements"
  },
  {
    "objectID": "posts/2024-12-29-setting-up-this-website.html",
    "href": "posts/2024-12-29-setting-up-this-website.html",
    "title": "Migrating this website from blogdown to Quarto",
    "section": "",
    "text": "From mid-2020 to late 2024, my personal website was built and rendered using the blogdown package.\nIn its first years (2016-ish to the spring of 2020, when I took advantage of some lockdown-induced free time to switch to blogdown), my personal website was built and hosted through Squarespace and their point-and-click interface. Squarespace is incredibly powerful if you want more sophisticated site features (floating navbars, clicking and dragging to set up complex text box layouts, etc), but I always felt that it was a bit overkill feature-wise (and subscription-wise) for an aesthetically pleasing, yet ultimately information-light website. Further, I hadn’t integrated my Squarespace site with any blogging tools, so I wasn’t able to have much regularly updated content beyond adding things onto the CV section of my About Me page.\nI’d been thinking of moving to a self-maintained static site for a while, especially one that would easily let me post R Markdown blog posts to the internet. I’m happy to trade in a less impressive theme for blog post compatibility with R Markdown. In case others might want to read my R Markdown ramblings, I want to put them online!\nAfter a couple days of not insignificant headache, I got the website up and (mostly) running! Many of these headaches stemmed from the fact that blogdown is built on the Hugo static site generator–essentially, what blogdown does is provide an interface to knit R Markdown files to Hugo-compatible vanilla Markdown files, and then use Hugo to render those to a website of static HTMLs.\nIf you stick with the default blogdown site theme, you’re basically good to go. However, if you want to use another Hugo theme that hasn’t already been pre-formatted for blogdown, or modify any page layouts, you are going to have a TIME learning Hugo if you’re not already a web developer (which I most certainly am not!).\nFor a year or more now, I’ve been wanting to switch my website over to a different theme/layout. The theme I was using before, Terminal, was really nice and clean, but I had such a hard time updating any of the layouts to play nice with both Hugo and blogdown that I realized I was actively avoiding a big website overhaul because it felt so daunting to set aside the time to do it.\nHowever, I found myself with some free time during the 2024 winter holiday, and I thought–why don’t I try migrating my website over to a Quarto website? Since 2022, when the first stable version of Quarto was released, I had started playing with it instead of R Markdown just to see what it was like. I quickly found that Quarto was similar enough in syntax to R Markdown that I didn’t have to learn a whole new language from scratch. Further, Quarto’s different built-in rendering formats are… amazing. Without installing any additional packages to specially format, you can render a .qmd to a single-page document, interactive slides, or a multi-page static website!\nFinally, as I was Googling guides for moving a website from blogdown to Quarto, I saw multiple people whose blogs/R packages I’ve used before 12345 had migrated in the same way, which convinced me!\nThe blog posts I’ve referenced in the footnotes (and the posts they themselves reference) all do a great job of covering different strategies for migrating content over. I encourage you to check through those posts for more details. On this page, I’m going to be pretty brief about my general migration process, but I’ll go into a little more detail about specific features I finagled if I didn’t see them described better by someone else already."
  },
  {
    "objectID": "posts/2024-12-29-setting-up-this-website.html#create-brand-new-repo-for-quarto-website",
    "href": "posts/2024-12-29-setting-up-this-website.html#create-brand-new-repo-for-quarto-website",
    "title": "Migrating this website from blogdown to Quarto",
    "section": "Create brand-new repo for Quarto website",
    "text": "Create brand-new repo for Quarto website\nFirst, I used the RStudio Quick Start to create a template Quarto website in a separate project folder from my original blogdown website folder."
  },
  {
    "objectID": "posts/2024-12-29-setting-up-this-website.html#copy-old-rmd-page-content-into-new-qmd-files",
    "href": "posts/2024-12-29-setting-up-this-website.html#copy-old-rmd-page-content-into-new-qmd-files",
    "title": "Migrating this website from blogdown to Quarto",
    "section": "Copy old Rmd page content into new qmd files",
    "text": "Copy old Rmd page content into new qmd files\nThen, I copied over some of the simpler pages on my website, starting with the landing page and about page. These pages were written in R Markdown for my blogdown website, but I changed the file endings to .qmd to save them as Quarto Markdown instead. (This isn’t strictly necessary, as Quarto websites can still render Rmd source files, but I figured it was a nice opportunity to update them.) These pages had no chunks of R code in them, so I didn’t have to alter any chunk syntax from Rmd to qmd. I only had to slightly alter the YAML headers to make sure they had the expected fields/field names.\nI organized the qmd files according to how I wanted the URL structure to come out. (For example, I put the sub-pages of my teaching portfolio into a folder called teaching, while my landing page, about page, research page, and game show pages are all in the project root folder.)\nAs Andreas Handel6 and Meghan Hall7 describe, the way that blogdown (by way of Hugo) and Quarto transform source folder structure into web page structure are somewhat different. Quarto does it the “naive” way (the source folder structure gets directly translated into the website URL structure). Hugo does it differently (source content in Markdown files you want to render has to be in a content folder, while source content you want to publish as-is has to be in a parallel static folder, but the resulting URL structure combines across the two), which again, has been overkill for my needs."
  },
  {
    "objectID": "posts/2024-12-29-setting-up-this-website.html#copy-non-rendered-content-images-etc-into-folders",
    "href": "posts/2024-12-29-setting-up-this-website.html#copy-non-rendered-content-images-etc-into-folders",
    "title": "Migrating this website from blogdown to Quarto",
    "section": "Copy non-rendered content (images, etc) into folders",
    "text": "Copy non-rendered content (images, etc) into folders\nFor any locally saved images I wanted to embed, I saved them into a sensible folder (either a page-dedicated folder like research-files/ for files on my research page, or misc-files/ for assorted images) and then embedded them using Markdown syntax, always making sure to link them using relative paths starting from the qmd location, not the project root (Markdown standard, not Quarto-specific)."
  },
  {
    "objectID": "posts/2024-12-29-setting-up-this-website.html#set-up-navigation-bar",
    "href": "posts/2024-12-29-setting-up-this-website.html#set-up-navigation-bar",
    "title": "Migrating this website from blogdown to Quarto",
    "section": "Set up navigation bar",
    "text": "Set up navigation bar\nThis was the first point at which I found Quarto way easier to deal with than Hugo.\nBoth website generators use a markup language config file (either YAML or TOML) to specify which pages appear on the navbar, and in what order.\nFor me, Hugo syntax was/is major overkill here. Even though the Hugo theme I was using had a template navbar setup that I could copy and paste new entries for, I still felt like I had no idea what I was doing.\nQuarto navbar setup was way easier. At the core, the only required items to specify are which pages (using filenames–easy!) you want to put on the navbar, and in what order.\nI was able to put pages on my navbar in the same order as in my personal website pretty easily. I also used the hybrid top navbar + sub-sidebar setup to add a sub-menu for the different components of my online teaching portfolio under the Teaching tab."
  },
  {
    "objectID": "posts/2024-12-29-setting-up-this-website.html#use-a-built-in-quarto-layout-for-my-about-page",
    "href": "posts/2024-12-29-setting-up-this-website.html#use-a-built-in-quarto-layout-for-my-about-page",
    "title": "Migrating this website from blogdown to Quarto",
    "section": "Use a built-in Quarto layout for my About page",
    "text": "Use a built-in Quarto layout for my About page\nAs a little bonus, I found that Quarto comes with some pre-made templates for “about me” pages. They’re super easy to use–they only require setting an option in the YAML header! (See if you can figure out which template I settled on.)"
  },
  {
    "objectID": "posts/2024-12-29-setting-up-this-website.html#set-up-blog-posts-section",
    "href": "posts/2024-12-29-setting-up-this-website.html#set-up-blog-posts-section",
    "title": "Migrating this website from blogdown to Quarto",
    "section": "Set up blog posts section",
    "text": "Set up blog posts section\n\nSet up blog listing landing page\nBecause I was setting up my blog as a component of a broader website, and not a standalone website, I did not use the RStudio Quick Start option for a Quarto blog, as that would have created the blog in a whole new project folder. Instead, I set up the necessary components manually, following the Quarto docs.\nFirst, I created a qmd for the listing page of blog posts that I wanted visitors to arrive at when they click the “Blog” link in the navbar. I named it posts.qmd (doesn’t really matter as long as you point the navbar to the right page in _quarto.yml) and copied and pasted the example YAML header from the Quarto docs into that page. I wanted the page to have the auto-generated listing and nothing else, so posts.qmd has only the YAML header in it. The options under the listing: YAML option are what direct Quarto to auto-generate a list of blog posts.\n\n\nSet up folder for posts\nThe contents: sub-option of the listing: option allows you to specify which files show up in the listing page. I went with the example setup option: contents: posts, which puts every qmd in a folder called “posts” on the listing page.\n(This is similar to how Hugo behaves, but the difference is that Hugo basically forces you to use a folder called “posts”, while Quarto lets you arbitrarily specify what files/folders you want to include, using path/wildcard syntax.)\n\n\nMigrate blog post Rmds to qmd\nHere, I did basically the same thing as I did when transferring my other Rmds to qmd. I copied my old Rmd blog post source files into the posts subfolder of my new Quarto website, and edited the YAML headers to make sure all of the settings were Quarto-compatible. The only setting I really had to change was from the blogdown tags: option to the Quarto categories option for tagging blog posts.\nBecause many of my blog post Rmds actually had executable R code, this time I made sure that all Rmd code chunks were re-written into Quarto syntax.\nI then set the execute: freeze: option to auto in the _quarto.yml website config file, so that blog posts (and all pages actually) with executable code would cache their outputs and not re-render unless the source file changed. That way large simulations don’t need to re-run!\nexecute:\n  freeze: auto"
  },
  {
    "objectID": "posts/2024-12-29-setting-up-this-website.html#adding-my-vitae-cv-and-setting-it-not-to-render-with-quarto",
    "href": "posts/2024-12-29-setting-up-this-website.html#adding-my-vitae-cv-and-setting-it-not-to-render-with-quarto",
    "title": "Migrating this website from blogdown to Quarto",
    "section": "Adding my vitae cv (and setting it not to render with Quarto)",
    "text": "Adding my vitae cv (and setting it not to render with Quarto)\nMy CV document was previously written as R Markdown and rendered using the vitae package[^As of writing, vitae 0.6.0 has a bug that causes markdowncv templates not to render. I’m continuing to use 0.5.4 until that bug is fixed.] This rendering engine uses different themes than the Quarto website, so I had to make sure to tell Quarto not to render my CV as a regular website content page. Instead, I make sure to knit my CV page directly to HTML myself when I edit the Rmd.\nI followed Quarto’s instructions for excluding my cv.Rmd from the auto-render, so that my custom-knitted vitae HTML wouldn’t get overwritten with Quarto’s version every time I re-render my website. However, cv.html still gets committed to the gh-pages branch for hosting, with all of the themed pages!"
  },
  {
    "objectID": "posts/2024-12-29-setting-up-this-website.html#re-pointing-my-old-squarespace-domain-name",
    "href": "posts/2024-12-29-setting-up-this-website.html#re-pointing-my-old-squarespace-domain-name",
    "title": "Migrating this website from blogdown to Quarto",
    "section": "Re-pointing my old Squarespace domain name",
    "text": "Re-pointing my old Squarespace domain name\nTo hook a GitHub Pages site to a custom domain name, you have to set up two steps:\n\nTell your domain name system (DNS) provider to point your domain name to your GitHub Pages URL\nTell your GitHub Pages site to use a custom domain\n\n\nPointing Squarespace DNS to GitHub Pages URL\nThe Squarespace website management feature that I opted to keep was domain name management. Squarespace allows you to buy an available domain name directly through their own site editor, so I’d previously bought monicathieu.com for my old website. I followed the instructions on Squarespace’s help pages to point my existing domain to a non-Squarespace site, so that when you go to monicathieu.com, it shows you whatever’s hosted at monicathieu.github.io instead. This takes care of the first half of the domain connection.\n\n\nGet GitHub Pages URL to use new domain\nIn order to complete the connection, I followed GitHub’s instructions to set www.monicathieu.com as the “custom domain” option in the GitHub Pages GUI settings for my repository. Then, instead of creating a new CNAME file per their instructions, I copied my existing CNAME file from my old personal website repo into the project root folder of my new website. Finally, I deleted the CNAME file from the gh-pages branch of my old website repo, so that there would be only one GitHub Pages website attempting to redirect to www.monicathieu.com ."
  },
  {
    "objectID": "posts/2024-12-29-setting-up-this-website.html#footnotes",
    "href": "posts/2024-12-29-setting-up-this-website.html#footnotes",
    "title": "Migrating this website from blogdown to Quarto",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRob Hyndman↩︎\nMeghan Hall: She details other custom layout features she had to finagle, like using a custom layout for her blog post listing, to migrate her website content from blogdown to Quarto.↩︎\nArt Steinmetz of Outsider Data↩︎\nJoseph Chou of Incidental Findings. His experience of Hugo themes’ blogdown compatibility breaking resonates with me, heh.↩︎\nAndreas Handel: He also details several specific non-default features he had working on his old blogdown site that he re-implemented in Quarto.↩︎\nAndreas Handel: He also details several specific non-default features he had working on his old blogdown site that he re-implemented in Quarto.↩︎\nMeghan Hall: She details other custom layout features she had to finagle, like using a custom layout for her blog post listing, to migrate her website content from blogdown to Quarto.↩︎"
  },
  {
    "objectID": "teaching/teaching-impact.html",
    "href": "teaching/teaching-impact.html",
    "title": "Teaching impact",
    "section": "",
    "text": "Collected below are a series of free-form testimonials about my teaching effectiveness. For my instructorships of record and teaching assistantships, I have excerpted comments about various aspects of my teaching from official teaching evaluations (original documents available upon request). I note that for my coding boot camp instructorships, students were not prompted to give instructor-specific feedback, so I have fewer student comments from those teaching experiences, but I have included a few testimonials that I do have, where relevant.\nComments are organized into three themes: my skills in breaking down complex topics, my energy in the classroom, and my clear, prompt, and actionable formative written feedback on assignments.",
    "crumbs": [
      "Teaching impact"
    ]
  },
  {
    "objectID": "teaching/teaching-impact.html#clear-handling-of-complex-topics",
    "href": "teaching/teaching-impact.html#clear-handling-of-complex-topics",
    "title": "Teaching impact",
    "section": "Clear handling of complex topics",
    "text": "Clear handling of complex topics\n\nDr Thieu did a very excellent job at making sure we understood the course information without feeling too overwhelmed or confused. (Statistics II in Psychology, Spring 2024, Spelman College)\n\n\nProfessor Thieu knows her stuff! You can tell she really knows the subject matter, and can answer questions in a relatable modern way. (Science of Psychology: Explorations & Applications, Summer 2022, Columbia University)\n\n\nI was very impressed how [she was] able to keep an eye on the chat, walk through the material, and switch to focusing on areas students were struggling, and in an absolutely relaxed way, which given that the material was new to people, was the perfect approach for a positive learning environment. (Supervisor comment, Foundations for Research Computing bootcamp, Winter 2022, Columbia University)\n\n\nOverall, Monica’s pacing was superb. She explained every step in great detail without lingering too long on any one point. Because she explained everything she typed, it was very easy not only to understand how the results were attained, but also why each step was chosen over any other option. Such clear explanations help students retain the information and, hopefully, make similar connections for themselves in their future projects. (Teaching observation, Wrangling Multilevel Data in the Tidyverse coding workshop, Spring 2022, Columbia University)\n\n\nMonica’s teaching and discussion leading skills are great. She was very friendly and facilitated really fun and interactive class activities to introduce some of the topics we were discussing and was also great at helping to explain concepts when asked about them in class. (Science of Psychology: Explorations & Applications, Spring 2021, Columbia University)\n\n\nMonica was a great TA: very approachable, clearly walked us through assignment requirements and expectations, taught us to code R in a straightforward and helpful manner, and maintained a positive and enthusiastic attitude throughout. (Research Methods, Fall 2019, Columbia University)\n\n\nShe also took on the daunting job of teaching non-CompSci students the basics of coding in R, and did it very well. I actually ended up enjoying the coding parts of the lab sections. (Research Methods, Fall 2018, Columbia University)\n\n\nMonica Thieu is a fantastic instructor, I never felt like I didn’t understand something or couldn’t do an assignment for class because Monica was great at explaining the material and finding new ways of understanding the material. (Research Methods, Fall 2018, Columbia University)\n\n\nMonica was a wonderful TA. She was very knowledgeable and good at explaining things, and was very available to provide help that was indeed helpful. She was also engaging in terms of leading discussions. (Research Methods, Fall 2018, Columbia University)",
    "crumbs": [
      "Teaching impact"
    ]
  },
  {
    "objectID": "teaching/teaching-impact.html#energy-and-enthusiasm",
    "href": "teaching/teaching-impact.html#energy-and-enthusiasm",
    "title": "Teaching impact",
    "section": "Energy and enthusiasm",
    "text": "Energy and enthusiasm\n\nShe was very engaging and promoted an open/welcoming environment where everyone felt safe to participate. (Science of Psychology: Explorations & Applications, Summer 2022, Columbia University)\n\n\nMonica kept my enthusiasm up by validating how I felt about programming environments while also conveying information in a way that encouraged asking questions and problem-solving. (Foundations for Research Computing bootcamp, Winter 2022, Columbia University)\n\n\nMonica was a very clear speaker, and she had a very uplifting and upbeat energy. (Science of Psychology: Explorations & Applications, Spring 2021, Columbia University)\n\n\nShe’s an amazing leader and teacher. She takes charge and I can definitely see her becoming a teacher one day, she just has the natural instincts of one. (Science of Psychology: Explorations & Applications, Spring 2021, Columbia University)\n\n\nMonica is such a fun and knowledgeable TA, I anticipate she will be a great teacher if she chooses to do so. She is very organized and manages to provide prompt feedback on assignments and foster a fun and engaging classroom environment. (Science of Psychology: Explorations & Applications, Spring 2021, Columbia University)\n\n\nMonica is a natural teacher and it’s VERY apparent that she has a passion for research. She’s absolutely fantastic! (Research Methods, Fall 2019, Columbia University)\n\n\nMonica is the best TA I’ve ever had! She’s easygoing, was super passionate about the topic (which I don’t think is common in a Research Methods TA), and was always well prepared for leading sections. (Research Methods, Fall 2018, Columbia University)",
    "crumbs": [
      "Teaching impact"
    ]
  },
  {
    "objectID": "teaching/teaching-impact.html#clear-and-prompt-written-feedback",
    "href": "teaching/teaching-impact.html#clear-and-prompt-written-feedback",
    "title": "Teaching impact",
    "section": "Clear and prompt written feedback",
    "text": "Clear and prompt written feedback\n\nThe instructor provides really helpful feedback on the problem sets. (Statistics II in Psychology, Spring 2024, Spelman College)\n\n\nAlways helpful and a quick grader as well. Her feedback was also very helpful (Science of Psychology: Explorations & Applications, Spring 2021, Columbia University)\n\n\nMonica’s feedback has always been very informative and helpful in terms of knowing what you might’ve missed/haven’t understood in the assignments. (Science of Psychology: Explorations & Applications, Spring 2021, Columbia University)\n\n\nHer feedback on my assignments was always helpful and clear. (Research Methods, Fall 2019, Columbia University)\n\n\nShe returned assignments promptly and was fair in her grading, explaining any deductions. (Research Methods, Fall 2018, Columbia University)",
    "crumbs": [
      "Teaching impact"
    ]
  },
  {
    "objectID": "teaching/teaching-statement.html",
    "href": "teaching/teaching-statement.html",
    "title": "Teaching philosophy",
    "section": "",
    "text": "As a teacher-scholar, my teaching development has been intertwined with my development as a researcher. In particular, I have learned about the ways in which effective research and teaching do not come from “eureka” moments of intellectual brilliance, but, instead, depend on thoughtful planning and structures that support success from the bottom up. Just as rigorous study design and statistical analysis are necessary to answer a research question, positive early experiences and supportive course structures are necessary for students to enter a discipline and thrive within it. I focus on designing and teaching classes where I can inspire new students to study psychology and scientific computing. In those classes, I implement accessible course policies to sustain student success throughout the semester. Finally, I maintain comprehensive course materials to standardize my pedagogy and share those materials to support my peers’ teaching.\nI strive to nurture novice scientists’ critical thinking skills. Intro-level instructors have the privilege and responsibility of serving as the welcome committee for students entering a new academic discipline. Psychology in particular can serve both as a gateway to further scientific study for students curious about human behavior, and as a core college science experience for students fulfilling breadth requirements. I am committed to balancing the needs of these two groups of students through intro psych classes that are rigorous yet accessible. For example, I collaborated with Dr. Caroline Marvin in developing a new team-based introductory psychology course at Columbia University (“The Science of Psychology: Explorations & Applications”) taught for the first time in spring 2021. I helped select readings, develop lesson plans, and draft assignments, paying special attention to craft learning objectives and structure assignments in a way that was simultaneously appropriate for prospective psychology/neuroscience majors and for general-education students. For instance, I helped build units around applied “challenges” that grouped psychological concepts based on real-world significance. One unit combined concepts from perception and infant/child development, which are not commonly grouped together in introductory psychology courses or textbooks, in the context of masked in-person vs. remote learning strategies for COVID safety in schools (see Thieu et al., 2022, Journal of College Science Teaching for a summary and reflection of our work). I then taught the course as instructor of record in summer 2022, further refining the course to take advantage of summer course structure. During the class on perception, groups of students broke down the results and figures from papers on emotion recognition from face vs. body, and during the class on development, groups practiced writing a “grant” to extend findings from rodent research on infant-maternal attachment to humans. During the final class of the unit, groups used evidence from pre-class readings and in-class activities to propose a hypothetical elementary school’s COVID policy, arguing for masked in-person, remote, or hybrid learning. I leveraged the longer summer class meetings to allow students to engage more deeply with the material via the activities, all while I was able to assess their learning by observing their group work. By the end of the course, students reflected positively on the skills they acquired and stated plans to apply course content to their work in other majors.\nGoing beyond content, I build accessibility into my course policies to sustain student success. For example, psychology students entering quantitative courses may lack experience using scientific computing tools like R to analyze data, but they can build those skills given sufficient support. In this case, an accessible grading structure is one that will scaffold students’ learning by rewarding incremental progress and learning from feedback, without penalizing students for early mistakes. As a curriculum developer and instructor, I have designed assignment submission and grading policies that allow students flexibility in managing their own deadlines, without penalizing students who need more practice but ultimately achieve the same learning objectives. As instructor for Stats II in Psychology at Spelman College in spring 2024, I designed a two-stage system where each weekly problem set was eligible for full-credit revisions, with no late or resubmission penalty, if revised within one week of receiving the initial grade and feedback. I designed this structure to incentivize students to engage with my feedback and improve their work to meet learning objectives, without penalizing students for first-round mistakes. At the same time, the revision structure allowed me to grade more rigorously on the first pass and hold students to higher standards, knowing that they would have the opportunity to respond to my feedback for full credit. Finally, the time-limited nature of the problem set revisions encouraged students to stay on top of their work through the course, while protecting me as the instructor from a buildup of revision submissions at the end of the term. Over half of students completed at least one revision and raised their problem set grade, showing that students indeed made use of the system. In this way, I was able to leverage course logistics to support student learning, especially for a quantitative course where students might bring insecurity about their own abilities into the classroom.\nFinally, in order to codify effective teaching practices and make them repeatable across terms and instructors, I create and refine detailed teaching materials that persist and evolve after my involvement with a course. As a lab instructor for the Spelman College Women in STEM six-week intensive program for incoming first-years in summer 2023 and 2024, I developed and taught a project-based data science course. Students first completed a crash course in data cleaning and visualization techniques using R, and then each student conducted an independent data science analysis—designing their own research question, sourcing their own publicly available datasets, identifying and graphing patterns in their data, and presenting their results. I created all lesson plans and course materials natively in R and managed a course workspace in RStudio Cloud for students to complete exercises and conduct their analyses. The course’s R integration made it easy to format course materials for storage and sharing. For example, on my end, I published the syllabus, course schedule, and lecture notes online as a public website using GitHub and Netlify, both for student reference and to share with other instructors and interested parties. On students’ end, they were able to export their summer work and re-publish their projects in their own GitHub coding portfolios, bringing the technical integration full-circle. In this way, I applied my technical expertise not only to support student learning during a single course but also to create a persistent store for course materials on which my peers and I can build in the future.\nIn summary, I work to make the discipline I love interesting and accessible to students through courses on psychological concepts, statistics, and scientific computing. I design those classes not only to inspire students through content but to set them up for success through supportive course policies. In those courses, I support my own and my peers’ future teaching by creating, maintaining, and documenting teaching materials that save time, keep record of successful teaching strategies, and support future innovation.",
    "crumbs": [
      "Teaching philosophy"
    ]
  },
  {
    "objectID": "game-shows.html",
    "href": "game-shows.html",
    "title": "Game shows",
    "section": "",
    "text": "First things first…\n\nBuy a Team Ken shirt or other swag here to represent/embarrass Ken Jennings, the GOAT. All profits donated to the Pancreatic Cancer Action Network in honor of Alex Trebek.\n\nWhen I was little, instead of doing normal kid things, I would read piles upon piles of nonfiction books about history and science. Somehow, a lot of this academic detritus has collected and stuck in my head over the years (hopefully useful for an academic?), and when combined with embarrassing volumes of angsty mid-2000s pop-punk lyrics, it turns out I know a lot of random stuff. Eventually I had the bright (and foolish) idea to try to make something of this. Here you can see what I’ve gotten away with so far…\n\n(2012) Jeopardy! College Championship\nIn 2012, I was selected to represent the University of North Texas in the Jeopardy! College Championship. At the time, I was a senior at the Texas Academy of Math and Science, a program for high school juniors and seniors to enroll at UNT, live in a dedicated dorm, and take two years of college courses. I was a UNT sophomore by credits, so I was technically a college undergraduate. If it’s good enough for Jeopardy!, it’s good enough for me!\nThanks to some educated guessing and a robust knowledge of the day’s viral YouTube videos, I managed to win first place in the tournament, taking home $100,000 and an awkwardly top-heavy trophy. The money disappeared into college tuition as quickly as it came, but the trophy is still somewhere in my parents’ house.\n\n\n(2013) Jeopardy! Tournament of Champions\nAfter years of merely yelling answers at the TV from my couch, getting to play Jeopardy! on the real stage got me hooked. Luckily, in addition to the top winners from the current season of grown-up Jeopardy!, the winner of the College Championship gets an automatic spot in that year’s Tournament of Champions. I got to come back for one last chance to get schooled by actual trivia experts, where I lost in the first round, but I had so much fun in the process that it was easily worth it in the end.\n\n\n(2016) 500 Questions, season 2\nAfter finishing Jeopardy!, I accepted my fate as a forgotten has-been. Then, out of the blue, almost 3 years later, I got a Facebook message from a casting director recruiting contestants for a new prime-time trivia TV special. After an arcane months-long audition process (the details of which I must take to my grave, or the production lawyers will send me there ahead of schedule), I was miraculously selected to be the first “challenger” on Season 2 of 500 Questions, a show where you try to answer as many (up to 500!) questions as you can without missing three in a row. I clawed my way up to $28,000 of prize money, which has paid for a whole lot of snacks (but never enough!).\n\n\n(2019) Jeopardy! All-Star Games\nYou can take the girl off the game show, but maybe you can’t take the game show out of the girl. Jeopardy! had long since been a joke item at the very end of my CV, when one day I got an email from the producers (who I hadn’t heard from in years) asking if I was interested in being a part of “something special…” Uh, YES. And look what happened!\nFor Jeopardy’s first team-based tournament, I rounded out Team Ken with Ken Jennings (we meet again, see above) & Matt Jackson. Ken, Matt, and I did a bit of evidence-based studying for our return to the show—if you’d like to hear a bit about the types of data science you can do to dig into Jeopardy strategy, you can listen to this NPR Planet Money episode, where Ken and I (among others) shared a bit of our experience. After two weeks of some of the most impeccably played Jeopardy I have ever witnessed (in the audience, and occasionally at the podium), we split a solid $300,000 second place prize.\n\n\n(2023) Master Minds\nIn 2023, I was selected to join the team of sweet and smart trivia experts holding court on the Game Show Network show Master Minds. On the show, three contestants face off against three trivia experts, competing to be the last contestant and last expert to answer questions against each other for $10,000. Depending on the air schedule, if you’re lucky, you can catch me on weekdays at 6 pm answering questions, serving up fun facts, and very visibly reacting to questions I’m excited to answer.\n\n\n(2024) Jeopardy! Invitational Tournament\nFive years and one global pandemic after I was previously invited back to Jeopardy!, I was lucky enough to be one of the past champions selected to return for the first Invitational Tournament, where players from different generations of the show get to see if they can shake off the rust. I loved getting to see old friends from the All-Star Games, and meeting other players old and new! And while I lost my quarterfinal game on a tough Final Jeopardy wagering choice, I’m still proud of my performance.\n\n\nPhotos (poorly organized, sorry!)\nThis photo gallery is under construction.\n\nblogdown::shortcode(.name = \"gallery\",\n                    folder = \"gallery\")"
  },
  {
    "objectID": "posts/2020-11-12-nypd-ccrb-data/index.html",
    "href": "posts/2020-11-12-nypd-ccrb-data/index.html",
    "title": "Exploring the NYPD Misconduct Complaint Database",
    "section": "",
    "text": "My cohort-mate and fellow R wonk, Paul Bloom, and I presented these slides for Columbia Foundations for Research Computing. Our presentation focused on cleaning and plotting data on civilian allegations of NYPD misconduct from the New York Civil Liberties Union."
  },
  {
    "objectID": "posts/2020-11-12-nypd-ccrb-data/index.html#the-nypd-misconduct-complaint-database",
    "href": "posts/2020-11-12-nypd-ccrb-data/index.html#the-nypd-misconduct-complaint-database",
    "title": "Exploring the NYPD Misconduct Complaint Database",
    "section": "The NYPD Misconduct Complaint Database",
    "text": "The NYPD Misconduct Complaint Database\n\nFrom the NYCLU:\n\nThe NYPD Misconduct Complaint Database is a repository of complaints made by the public on record at the Civilian Complaint Review Board (CCRB). These complaints span two distinct periods: the time since the CCRB started operating as an independent city agency outside the NYPD in 1994 and the prior period when the CCRB operated within the NYPD. The database includes 323,911 unique complaint records involving 81,550 active or former NYPD officers. The database does not include pending complaints for which the CCRB has not completed an investigation as of July 2020."
  },
  {
    "objectID": "posts/2020-11-12-nypd-ccrb-data/index.html#our-slides",
    "href": "posts/2020-11-12-nypd-ccrb-data/index.html#our-slides",
    "title": "Exploring the NYPD Misconduct Complaint Database",
    "section": "Our slides",
    "text": "Our slides\n(For both of these slide previews, you can click inside and use the left & right arrow keys to navigate between slides.)\nPart 1: Data Cleaning\n\n\n\n\n\n\n\n\nPart 2: Plotting, with a focus on using ggalluvial to visualize complaints as they are ruled on by the CCRB, and then by the NYPD itself\n\n\n\n\n\n\n\n\n\nSource code\nThese slides were made with xaringan. They are hosted in their own GitHub repo should you like to clone the code yourself.\nThey are packaged with an renv lockfile that should allow you to download all the dependency packages to run the code with a few commands. Please note that the project was written primarily in R 4.0.3. If you have R &gt;= 4.0.0, renv::restore() should work smoothly to download our dependency packages, but if you have R 3.x.x you may not find it so easy (some of the dependency versions require 4.0.0 or above).\n\n\nHosting the slides and showing them on this page\nI used GitHub Pages’ hosting capabilities to host the slides directly from the main repo.\nAny GitHub repo can have its contents hosted and deployed using Pages. If it’s a lightweight repo with only a couple HTML pages, as the cu-nypd-ccrb-data repo is, select the “Deploy from a branch” option in the Pages subsection of the repo settings. Leave the selected deploy branch on the default branch–no need for a second gh-pages branch in this case.\nThe repo will deploy to YOUR-USERNAME.github.io/REPO-NAME/. The README will be rendered as the landing page by default if you don’t have a file named index.html. Your slides will be in there at YOUR-USERNAME.github.io/REPO-NAME/SLIDE-FILE-NAME.html! You can now reference that global link in any other website. Here, I’ve iframed the slides in from their home repo to show up on this page."
  },
  {
    "objectID": "posts/2020-11-12-nypd-ccrb-data/index.html#a-non-exhaustive-list-of-related-reports",
    "href": "posts/2020-11-12-nypd-ccrb-data/index.html#a-non-exhaustive-list-of-related-reports",
    "title": "Exploring the NYPD Misconduct Complaint Database",
    "section": "A non-exhaustive list of related reports",
    "text": "A non-exhaustive list of related reports\nWe are by no means the only people to probe this data, or related older data. Please see the following for more:\n“Mission Failure: Civilian Review of Policing in New York City – Summary of Findings”, NYCLU, 2006\n“Police Officers Rarely Disciplined by NYPD for Misconduct”, WNYC, Aug 27 2014\n“Newly Released Data Shows 1 Out Of Every 9 NYPD Officers Has A Confirmed Record Of Misconduct”, Gothamist, July 28 2020\n“The NYPD Is Withholding Evidence From Investigations Into Police Abuse”, ProPublica, Aug 17 2020\n“Why The Majority Of NYPD Misconduct Complaints End Up ‘Unsubstantiated’”, Gothamist, August 18 2020\n“323,911 Accusations of N.Y.P.D. Misconduct Are Released Online”, New York Times, Aug 20 2020\n“What happened to NYPD officers who were charged with misconduct? They were promoted or paid more.” Columbia Spectator, Sept 22 2020"
  },
  {
    "objectID": "posts/2020-08-15-prepare-teach-r-lesson.html",
    "href": "posts/2020-08-15-prepare-teach-r-lesson.html",
    "title": "How to prepare and teach an R lesson",
    "section": "",
    "text": "This blog post contains a written version of my speaker notes for my 20-minute 2020 NYC R conference talk. Since the talk time is on the shorter side, I trimmed a few things out of the talk that I would have liked to include with more time. The speaker notes for the unabridged version are included here for your reference."
  },
  {
    "objectID": "posts/2020-08-15-prepare-teach-r-lesson.html#what-is-backward-design",
    "href": "posts/2020-08-15-prepare-teach-r-lesson.html#what-is-backward-design",
    "title": "How to prepare and teach an R lesson",
    "section": "What is backward design?",
    "text": "What is backward design?\nWhen writing code, be it a helper function, a data processing script, or a whole package, there’s a common logical order we follow.\n\nDefine function output\nWrite unit test\nWrite function code\n\nWriting the code itself ideally comes last. We can only write the code once we know what it’s supposed to do, and how we’ll verify that. If you started writing code before you decided what it was supposed to do, the code would go nowhere!\nBackward design takes that same logic and applies it to teaching. The “usual” method of planning lessons starting from activities and demonstrations can be referred to as “forward design.” If planning lessons starting from activities is “forward design”, then backward design involves the following steps:\n\nDefine learning outcomes\nDefine evidence of learning\nBuild lesson\n\nThis looks a lot like the logical process of coding. Both coding logic and backward lesson design aim to answer the following questions, in order:\n\nWhat should code/learners be able to do?\nHow will we know code/learners can do that?\nHow will we equip code/learners with the ability to do that?\n\nYou already know this logic! Lesson planning is conceptually similar to writing code. You don’t need to acquire a whole new knowledge base from scratch to be able to plan R lessons. You only need to learn how to relate concepts from coding logic to backward lesson design, and that’s exactly what we’re going to do.\nNext, we’ll dig more into each of these three steps of backward design as they might manifest in an R coding lesson."
  },
  {
    "objectID": "posts/2020-08-15-prepare-teach-r-lesson.html#define-outcomes",
    "href": "posts/2020-08-15-prepare-teach-r-lesson.html#define-outcomes",
    "title": "How to prepare and teach an R lesson",
    "section": "Define outcomes",
    "text": "Define outcomes\nDefining the learning outcome, or objective (used interchangeably from here on out), is conceptually similar to defining the output of a function, script, or entire package that one might write. It’s the answer to the following question:\nWhat will learners be able to do when the lesson is finished?\nYou have to be specific when spelling out learning goals for them to be most effective. Just like it’s harder to write a function when you haven’t delineated what you want it to do, it’s harder to prepare a lesson when you haven’t outlined what you want it to teach.\nLess specific (worse):\n\nLearners will understand how tidyr, dplyr, and ggplot2 help them clean and explore their data\n\nMore specific (better):\n\nLearners will be able to:\n\nuse tidyr’s pivot_wider() and pivot_longer() to reshape data for easier analysis\nuse dplyr’s group_by(), count(), and summarize() functions to generate summary statistics\nuse ggplot2 to generate exploratory scatterplots of data\n\n\nA few tips on making your R lesson objectives more specific and effective:\n\nUse active verbs. “Learners will be able to use … to …” is a good starter\nIf you’re focusing on a specific package, name that package\nIf you’re focusing on specific functions, name those functions\nMake it more verbose (but don’t go overboard!)\n\nNote: Most learning goals in lessons about R will be to “use” or “implement” certain functions or techniques. However, depending on the topic, the goal may be something lower, like to “recognize” or “recall” which function does what, or something higher, like to “evaluate” or “critique” code for style and speed. If you’re interested in reading more about the different types of learning goals one might set for a lesson, and which one best fits the level of learning you want, Bloom’s Revised Taxonomy is a helpful framework."
  },
  {
    "objectID": "posts/2020-08-15-prepare-teach-r-lesson.html#define-evidence-of-learning",
    "href": "posts/2020-08-15-prepare-teach-r-lesson.html#define-evidence-of-learning",
    "title": "How to prepare and teach an R lesson",
    "section": "Define evidence of learning",
    "text": "Define evidence of learning\nOnce you’ve outlined your desired learning objectives, the next step is to define acceptable evidence of learning. How will you determine that learners can indeed accomplish what the learning objective says they can?\nAgain, in programming and teaching, we must test to confirm that our code (or our learners) have accomplished the objective.\nIn coding, this can be as simple as running your code multiple times under different conditions to make sure the output looks right, or as elaborate as a full unit testing system (we’ll touch on this later!).\nIn a semester-long programming course, this would be the final assignment or exam. In a 5-minute tutoring session, this might involve watching your colleague write code that solves the problem they came to you for. In every case, the only way we can know that learners have reached the objective is by testing their ability to demonstrate it.\nNote: the final assessment example below was omitted from the talk for time.\nFor example, the final test for a lesson with the learning objectives shown above might be:\n\nThe Orange dataset in R’s datasets package contains data for the ages (in days) and circumferences (in mm) of several orange trees, each measured at different timepoints.\n\nUse the appropriate pivot (_wider or _longer) function to pivot the data to have one row for each measurement, one column for the age of each tree, and one column for the circumference of each tree.\nUsing the long form of the data, generate a dataframe showing the number of measurements for each tree.\nUsing the long form of the data, generate a dataframe showing the minimum and maximum age and circumference for each tree.\nUsing appropriate ggplot2 functions, generate a scatterplot of tree circumference by age. Plot lines connecting the observations for each tree. The points and lines for each tree should be different colors. Add an informative title and axis labels.\n\n\nThere doesn’t have to be one test question for each objective, but it’s a good place to start to make sure you have appropriate coverage. Just like you want high code coverage when testing your own code, you want high objective coverage with your final assessment, to ensure that you’re testing learners’ ability to execute every objective that you want them to accomplish. No matter how many prompts you include in your assessment, make sure that tests are written specifically enough that a learner who has reached the objective would be able to answer every question acceptably. Vague prompts don’t “give away the answer,” they give enough information for learners to get to the answer.\nThere can (and often should) be a back-and-forth process between defining learning outcomes and defining tests of learning. Sometimes, you might find that your test isn’t testing exactly what you think learners should be able to do. You might add more to your test, or change what’s already in it, so that it better fits the learning outcome. Other times, you might have a sense that the test itself correctly expresses what you want learners to come away with. In those cases, you might re-write the learning outcome so that it better fits the test. The key is to plan with intention either way.\n\nFormative assessment: more smaller tests\nThe final assessment is of course essential to determining whether learners have met your objective. But, by design, it happens at the end of the lesson. What if learners make critical mistakes on the final assessment, showing that they haven’t actually reached the objective? At that point, the lesson is over, and you don’t have any time remaining to get learners back on track. :(\nEnter formative assessment! When teaching, you have many chances to assess learning before you get to the end of the lesson. Formative assessments are frequent and specific tests of learner understanding during teaching.\nFormative assessments in teaching are like unit tests in coding:\n\nboth identify bugs in learner understanding/code functionality\nboth are built-in to the teaching/development process\nboth enable in-the-moment remediation/debugging\nboth encourage more effective lesson/code style (more on this later)\nif learners/code fail, it’s on the road to getting it right next time!\n\nHow do you implement formative assessments? Consider the example lesson we’ve started mocking up. In this lesson, to teach toward the second objective, we might calculate means and standard deviations of petal/sepal lengths and widths in the iris dataset. We would live-code this chunk and learners would follow along. The final code, and its finished output, might look something like below:\n\niris %&gt;%\n  rename_with(tolower) %&gt;% \n  pivot_longer(cols = -species,\n               names_to = c(\"part\", \".value\"),\n               names_sep = \"\\\\.\") %&gt;% \n  group_by(species, part) %&gt;% \n  summarize(across(everything(), list(mean = mean, sd = sd)))\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 6 × 6\n# Groups:   species [3]\n  species    part  length_mean length_sd width_mean width_sd\n  &lt;fct&gt;      &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n1 setosa     petal        1.46     0.174      0.246    0.105\n2 setosa     sepal        5.01     0.352      3.43     0.379\n3 versicolor petal        4.26     0.470      1.33     0.198\n4 versicolor sepal        5.94     0.516      2.77     0.314\n5 virginica  petal        5.55     0.552      2.03     0.275\n6 virginica  sepal        6.59     0.636      2.97     0.322\n\n\nThe pipe chain above contains four functions. If a learner makes an error in calling any one of these functions, the entire pipe chain would fail. Thus, when we ask learners to write the whole block of code only once at the end of the module, it’s less obvious where an error might come from, both to instructors and learners.\nFor example, if a learner wrote the following answer:\n\niris %&gt;%\n  rename_with(tolower) %&gt;% \n  pivot_longer(cols = -species,\n               names_to = c(\"part\", \".value\"),\n               names_sep = \"\\\\.\") %&gt;% \n  group_by(Species, part) %&gt;% \n  summarize(across(everything(), list(mean = mean, sd = sd)))\n\nError in `group_by()`:\n! Must group by variables found in `.data`.\n✖ Column `Species` is not found.\n\n\nAn experienced instructor can read the error message and deduce that the error occurs inside of group_by(), and is likely to be a misspelling bug (or, perhaps more likely, was carried over from a version of the code where the column names were not coerced to lowercase). However, this error message may be opaque to the learner, and does little to help them identify their mistake themselves.\nFurther, this test requires the instructor to exert extra effort to trace the error. If multiple learners make unique errors in writing this block of code, the instructor must now take even more time to correct each learner’s unique mistake. Learners must also spend time waiting for the instructor, instead of progressing.\n\n\nTidy formative assessments\nThe function chains encouraged by tidyverse style lend themselves to regular formative assessments. Formative assessment check-ins can built in to the lesson after every function in the pipe chain is introduced.\nWe can reformat the code chunk above to have a formative assessment check-in for each verb. For example, here’s a couple ways we might test learners’ understanding of what named functions do inside of summarize(across()). We start with the block of code that learners will have already successfully written and run:\n\niris_long &lt;- iris %&gt;% \n  rename_with(tolower) %&gt;% \n  pivot_longer(cols = -species,\n               names_to = c(\"part\", \".value\"),\n               names_sep = \"\\\\.\") %&gt;% \n  group_by(species, part)\n\nThen, we might give specific instructions to write one additional line of code from scratch to summarize iris_long in a particular way.\n\nUsing across() inside summarize(), write a line of code to summarize the mean and SD of all possible measurements (length and width) for each iris species and anatomical part. Your output should have summary columns with “mean” or “sd” appended to them depending on what metric they are.\n\n\n# CODE GOES HERE\n\nAlternatively, we might provide sample code that learners can modify in a specific way to demonstrate learning of a bite-size concept. While this requires learners to generate less code from scratch, it can be faster during a lesson.\n\nChange the summarize() call on line 104 so the mean outputs have the suffix “_avg” instead of “_mean”.\n\n\niris_long %&gt;% \n  summarize(across(everything(), list(mean = mean, sd = sd)))\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 6 × 6\n# Groups:   species [3]\n  species    part  length_mean length_sd width_mean width_sd\n  &lt;fct&gt;      &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n1 setosa     petal        1.46     0.174      0.246    0.105\n2 setosa     sepal        5.01     0.352      3.43     0.379\n3 versicolor petal        4.26     0.470      1.33     0.198\n4 versicolor sepal        5.94     0.516      2.77     0.314\n5 virginica  petal        5.55     0.552      2.03     0.275\n6 virginica  sepal        6.59     0.636      2.97     0.322\n\n\nChecking for understanding after each new verb in the pipe chain has a few benefits:\n\nLearners get a natural pause after each new concept is introduced, to solidify understanding before adding more info\nWhen fewer lines of code are tested at once, learners’ misconceptions/mistakes are more likely to overlap, allowing the instructor to address confusions with the whole group, instead of one-on-one\nEach check-in can be repeated until 100% of learners pass, ensuring that when the next function is introduced, learners will all be caught up\n\n\n\nMultiple choice check-ins scale pretty well\nThe above check-in is written as a free-response prompt to “write code that does the job”. In small learner groups, this is the most flexible way to check for understanding. If they can write working code, then they’ve met the learning objective! However, this does require that the instructor either trust that everyone’s code works, or visually check each learner’s work to confirm the code behaves as intended. This can be time-consuming for larger groups.\nThere are other methods of formative assessment check-in that, while less comprehensive, scale much better to large groups of learners. My favorite of these are multiple choice questions:\n\nWhich of the chunks below will correctly return summarized mean and SD outputs, where the mean columns have the suffix “_avg”?\n\n\niris_long %&gt;% \n  summarize(across(everything(), list(mean = avg, sd = sd)))\n\n\niris_long %&gt;% \n  summarize(across(everything(), list(avg = mean, sd = sd)))\n\nThese can be implemented by copying screenshots or code text into an audience-response system like Poll Everywhere or Socrative (those two I know about because they’re geared to educators, but any polling app works!). Then, the instructor can quickly look at poll responses and gauge how many learners are choosing the correct answer.\nFurther, the distractor (incorrect) answers can also reveal specific misconceptions nearly as well as free-response check-ins. Write distractor answers to look plausible, featuring common mistakes. (Perhaps a mistake you once made when you were learning how to use this function!)\nIn this way, you know that learners are choosing the correct answer because they believe it is the right answer, and not because they believe all the other answers must be wrong. (In the second scenario, a learner might pass a check-in without actually learning the concept at hand!)\nAdditionally, to the previous point, do not write “trick” distractor answers, particularly those where you have not yet shown learners why such a mistake is incorrect. Testing learners on skills/concepts you haven’t taught them is a waste of testing time for you, and a waste of effort for learners."
  },
  {
    "objectID": "posts/2020-08-15-prepare-teach-r-lesson.html#build-lesson",
    "href": "posts/2020-08-15-prepare-teach-r-lesson.html#build-lesson",
    "title": "How to prepare and teach an R lesson",
    "section": "Build lesson",
    "text": "Build lesson\nThe beauty of backwards design is that once you’ve spelled out your learning objectives, and the tests you’ll use to verify that learners have reached those objectives, the lesson itself is nearly done. Ideally, once you’ve laid out your assessments, you will be able to see exactly what you need to demonstrate and explain to equip learners with enough knowledge to complete those assessments.\nThe core principle I try to rely on when crafting lessons is to teach what you need to teach: no more and no less. What does this mean? This encompasses several things, including:\nCalibrate to learners’ incoming skill level. What do you expect learners to be able to do already before they start your lesson? Start where learners currently are (don’t re-teach), and teach everything new that learners need to know to reach the objective (think through all the pre-reqs!).\nAllow your formative assessments to break the lesson into manageable chunks. If you have implemented the right number and scope of checkpoints, each check-in question should serve as the end to a bite-sized (5-10 min) piece of lesson. These bites are a good size for you, because you get to take breaks from speaking and survey learners’ progress, and a good size for learners, because they get to take a breather from new information and use the check-in question to practice what they’ve just learned.\nTeach less than you think you have time for. An instructor teaching a particular lesson for the first time is likely to find that the lesson simply takes way longer than originally planned. Running out of time before you’ve finished the lesson guarantees that learners won’t reach the objective! Plan for less time than you’ll actually have. The first time I teach a new lesson, I plan it for 2/3 of the allotted class time, and it usually comes out right.\nTeach one way to do things. One of the great (and terrible) features of R is that there’s often many, many different ways to solve a problem. While some might be less verbose, more generalizable, etc., than others, ultimately sometimes you get to two different techniques that differ only on personal taste. For example, there might be two different ways to teach the pivot_longer() technique demonstrated earlier.\nUsing names_sep to break up column names:\n\niris %&gt;% \n  rename_with(tolower) %&gt;% \n  pivot_longer(cols = -species,\n               names_to = c(\"part\", \".value\"),\n               names_sep = \"\\\\.\")\n\n# A tibble: 300 × 4\n   species part  length width\n   &lt;fct&gt;   &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 setosa  sepal    5.1   3.5\n 2 setosa  petal    1.4   0.2\n 3 setosa  sepal    4.9   3  \n 4 setosa  petal    1.4   0.2\n 5 setosa  sepal    4.7   3.2\n 6 setosa  petal    1.3   0.2\n 7 setosa  sepal    4.6   3.1\n 8 setosa  petal    1.5   0.2\n 9 setosa  sepal    5     3.6\n10 setosa  petal    1.4   0.2\n# ℹ 290 more rows\n\n\nUsing names_pattern to break up column names:\n\niris %&gt;% \n  rename_with(tolower) %&gt;% \n  pivot_longer(cols = -species,\n               names_to = c(\"part\", \".value\"),\n               names_pattern = \"(.*)\\\\.(.*)\")\n\n# A tibble: 300 × 4\n   species part  length width\n   &lt;fct&gt;   &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 setosa  sepal    5.1   3.5\n 2 setosa  petal    1.4   0.2\n 3 setosa  sepal    4.9   3  \n 4 setosa  petal    1.4   0.2\n 5 setosa  sepal    4.7   3.2\n 6 setosa  petal    1.3   0.2\n 7 setosa  sepal    4.6   3.1\n 8 setosa  petal    1.5   0.2\n 9 setosa  sepal    5     3.6\n10 setosa  petal    1.4   0.2\n# ℹ 290 more rows\n\n\nIn practice, there are valid arguments for using either method. Using names_sep to split along the period is less verbose, but is theoretically less flexible than spelling out the capture groups with names_pattern, in case there are future columns in the data that delimit with periods AND underscores.\nWhat is the learning goal here? If indeed a primary objective of the lesson is to be able to pivot many different arrangements of data, then students do need to learn about the similarities and differences between using names_sep and names_pattern to break up column names in service of the objective. However, if the objective is solely to be able to pivot data with multiple value columns using the \".value\" placeholder, then being able to compare, contrast, and choose between names_sep and names_pattern is not part of the learning goal. In this case, you as the instructor need to decide which technique you would rather students come away using, and teach only that technique.\nRelated to this point: don’t teach first principles unless the learning goal is first principles. I was guilty of this for a while! For example, when I used to teach vectorized operations like apply(), and later map(), I would lead with a whole section on for loops and why they were slow and unwieldy. When I learned how to take full advantage of R’s vectorized functions, I felt like I had woken up from a fog of for loops. I assumed this experience generalized to other people, and so when I taught those functions I would spend a lot of time making for loops sound like the black-and-white “before” section of an infomercial before the life-changing new product is revealed. However, I eventually received feedback that learners didn’t understand why I was spending so much time talking about for loops when I didn’t teach them, and some learners didn’t even know what for loops were before I mentioned them, so they were getting confused.\nWhile it is true that a first-principles comparison of for loops and vectorized functions is important to a full understanding of when vectorizing is and isn’t the best solution to a code problem, understanding first-principles isn’t necessary to be able to correctly use a vectorized function. Once I realized this, I was able to let go of a lesson module that was more nostalgic to me than instructive to learners.\n(I actually heard the above from Hadley Wickham! At Columbia I was lucky to be able to take Dr. Andrew Gelman’s Communicating Data course, where we heard from guest speakers like Hadley.)"
  }
]